{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from math import sqrt\n",
    "from math import log2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 txt 檔案\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "# 讀取資料\n",
    "data = load_data('glass.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將原始 list of list 型態的 data 轉換為 dict of list\n",
    "def table_to_column_dict(data, columns, convert_numeric = True):\n",
    "    # 初始化一個空的 dictionary，key 是 attributes 名稱，value 是空 list：\n",
    "    df_dict = {col: [] for col in columns}\n",
    "    # 對每一列資料逐欄掃描，同時把欄位名稱 (col) 跟對應值 (val) 配對起來\n",
    "    for row in data:\n",
    "        for col, val in zip(columns, row):\n",
    "            if convert_numeric:\n",
    "                try:\n",
    "                    val = float(val)\n",
    "                except ValueError:\n",
    "                    pass  # 若轉不了 float，就保持原樣（例如 Id）\n",
    "            df_dict[col].append(val)\n",
    "    return df_dict\n",
    "# 定義欄位名稱\n",
    "columns = [\"Id\",\"RI\",\"Na\",\"Mg\",\"Al\",\"Si\",\"K\",\"Ca\",\"Ba\",\"Fe\",\"class\"]\n",
    "df = table_to_column_dict(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = columns.copy()\n",
    "X.remove(\"Id\")\n",
    "X.remove(\"class\")\n",
    "y = 'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算 feature entropy\n",
    "def entropy(df,feature):  \n",
    "    att_value = df[feature]  # 取出 dict 的特定 attribute 的所有資料\n",
    "    value_count = Counter(att_value)          # 計算所有可能值的個數\n",
    "    total = len(att_value)\n",
    "    prob = [count / total for key,count in value_count.items()]  # 計算每個 attribute_value 的機率\n",
    "    return -sum(p * log2(p) for p in prob)\n",
    "\n",
    "# 計算特徵 X、Y 間的 Mutual Information\n",
    "def mutual_information(df,X, Y):\n",
    "    X_list = df[X]\n",
    "    Y_list = df[Y]\n",
    "    # 計算 X 和 Y 的熵\n",
    "    H_X = entropy(df ,X)\n",
    "    H_Y = entropy(df ,Y)\n",
    "    # 計算 X 和 Y 的聯合機率\n",
    "    joint_pairs = list(zip(X_list, Y_list))\n",
    "    joint_counts = Counter(joint_pairs)\n",
    "    total = len(X_list)\n",
    "    joint_prob = [count / total for key,count in joint_counts.items()]\n",
    "    H_X_Y = -sum(p * log2(p) for p in joint_prob)\n",
    "    return H_X + H_Y - H_X_Y\n",
    "    \n",
    "# 計算特徵 X、Y 的 symmetric uncertainty\n",
    "def cal_su(df,X,Y):\n",
    "    H_X = entropy(df,X)\n",
    "    H_Y = entropy(df,Y)\n",
    "    if H_X == 0 and H_Y == 0:\n",
    "        return 0\n",
    "    return 2 * (mutual_information(df,X,Y) / (H_X + H_Y))\n",
    "\n",
    "# 計算選取的特徵子集對於類別預測的 Goodness\n",
    "def Goodness(df,feature_subset,label):\n",
    "    su_X_C = 0\n",
    "    sum_su_X_Y = 0  \n",
    "    # 計算 feature_subset 內所有特徵對於類別值的 Symmetric uncertainty\n",
    "    su_X_C = sum(cal_su(df,X,label) for X in feature_subset)\n",
    "    \n",
    "    # 計算 feature_subset 內所有兩兩特徵間的 Symmetric uncertainty\n",
    "    for feature_i in feature_subset:\n",
    "        for feature_j in feature_subset:\n",
    "            sum_su_X_Y += cal_su(df,feature_i,feature_j)\n",
    "    if sum_su_X_Y == 0:\n",
    "        return 0\n",
    "    return su_X_C / sqrt(sum_su_X_Y)\n",
    "\n",
    "def forward_selection(df, X, y):\n",
    "    select_features = []    \n",
    "    best_score = 0.0        \n",
    "    remaining_features = X.copy()  \n",
    "    # 持續檢查直到沒有可以選擇的 feature\n",
    "    i = 1   \n",
    "    while(len(remaining_features) > 0):\n",
    "        scores = []  \n",
    "        for feature in remaining_features:\n",
    "            # temp_features 暫存此次循環的特徵組合 => 上回以選取好的最佳組合 select_features + 這回新選入的一個 feature\n",
    "            temp_features = select_features + [feature]\n",
    "            score = Goodness(df,temp_features,y)\n",
    "            # (目前的特徵組合, 新選進來的特徵, 此特徵組合的 Goodness)\n",
    "            scores.append((temp_features,feature,score))\n",
    "\n",
    "        # 依照 Goodness 排序\n",
    "        scores.sort(key=lambda x: x[2], reverse = True)  \n",
    "        best_new_score = float(scores[0][2])  \n",
    "        print(\"Forward Selection:\")\n",
    "        if(best_new_score > best_score):\n",
    "            best_score = best_new_score\n",
    "            select_features = scores[0][0]  # 更新成 Goodness 最優的 subset\n",
    "            if scores[0][1] in remaining_features:\n",
    "                remaining_features.remove(scores[0][1])  # 移除新選特徵\n",
    "            print(f\"Pass{i}: best_feature_subset = {select_features} , Goodness = {best_score}\")\n",
    "            i += 1\n",
    "        # 此輪中所有 feature_subset 的表現皆不如上一輪，Stop\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Final select features: {select_features}, Goodness = {best_score}\")\n",
    "\n",
    "def backward_selection(df, X, y):\n",
    "    select_features = X    \n",
    "    best_score = 0.0       \n",
    "    i = 1\n",
    "    # 持續檢查到選擇的 feature 只剩下一個\n",
    "    while(len(select_features) > 1):\n",
    "        scores = [] \n",
    "        for feature in select_features:\n",
    "            temp_features = select_features.copy()\n",
    "            # 每次移除一個 feature\n",
    "            temp_features.remove(feature)\n",
    "            score = Goodness(df,temp_features,y)\n",
    "            # (目前的特徵組合, 移除的特徵, 此特徵組合的 Goodness)\n",
    "            scores.append((temp_features,feature,score))\n",
    "\n",
    "        scores.sort(key = lambda x: x[2], reverse = True)  \n",
    "        best_new_score = float(scores[0][2]) \n",
    "        print(\"Backward Selection:\")\n",
    "        if(best_new_score >= best_score):\n",
    "            best_score = best_new_score\n",
    "            select_features = scores[0][0]  # 更新成 Goodness 最優的 subset\n",
    "            if scores[0][1] in select_features:\n",
    "                select_features.remove(scores[0][1])  # 移除特徵\n",
    "            print(f\"Pass{i}: best_feature_subset = {select_features} , Goodness = {best_score}\")\n",
    "            print(f\"remove feature: {scores[0][1]}\")\n",
    "            i += 1\n",
    "        # 此輪中所有 feature_subset 的表現皆不如上一輪，Stop\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Final select features: {select_features}, Goodness = {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with equal width discretization => width = 0.0022780000000000022\n",
      "[1.513428, 1.515706, 1.517984, 1.520262, 1.52254, 1.524818, 1.527096, 1.529374, 1.531652]\n",
      "=========================================================\n",
      "Na with equal width discretization => width = 0.6649999999999998\n",
      "[11.395, 12.06, 12.725, 13.39, 14.055, 14.719999999999999, 15.384999999999998, 16.049999999999997, 16.715]\n",
      "=========================================================\n",
      "Mg with equal width discretization => width = 0.449\n",
      "[0.449, 0.898, 1.347, 1.796, 2.245, 2.694, 3.1430000000000002, 3.592, 4.041]\n",
      "=========================================================\n",
      "Al with equal width discretization => width = 0.321\n",
      "[0.611, 0.9319999999999999, 1.2530000000000001, 1.574, 1.895, 2.216, 2.537, 2.858, 3.1790000000000003]\n",
      "=========================================================\n",
      "Si with equal width discretization => width = 0.5599999999999994\n",
      "[70.37, 70.93, 71.49, 72.05, 72.61, 73.17, 73.73, 74.28999999999999, 74.85]\n",
      "=========================================================\n",
      "K with equal width discretization => width = 0.621\n",
      "[0.621, 1.242, 1.863, 2.484, 3.105, 3.726, 4.3469999999999995, 4.968, 5.589]\n",
      "=========================================================\n",
      "Ca with equal width discretization => width = 1.076\n",
      "[6.506, 7.582, 8.658, 9.734, 10.81, 11.886, 12.962, 14.038, 15.114]\n",
      "=========================================================\n",
      "Ba with equal width discretization => width = 0.315\n",
      "[0.315, 0.63, 0.9450000000000001, 1.26, 1.575, 1.8900000000000001, 2.205, 2.52, 2.835]\n",
      "=========================================================\n",
      "Fe with equal width discretization => width = 0.051000000000000004\n",
      "[0.051000000000000004, 0.10200000000000001, 0.15300000000000002, 0.20400000000000001, 0.255, 0.30600000000000005, 0.35700000000000004, 0.40800000000000003, 0.459]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def equal_width(df,feature,bin_num):\n",
    "    # 計算每組 bin 區間\n",
    "    att_value = df[feature]\n",
    "    max_value = float(max(att_value))\n",
    "    min_value = float(min(att_value))\n",
    "    # 計算每組區間寬度\n",
    "    width = (max_value - min_value) / bin_num\n",
    "\n",
    "    bins = []  # 儲存每組區間範圍\n",
    "    # 計算每組區間數值範圍\n",
    "    for i in range(1,bin_num):\n",
    "        cut_point = min_value + i * width\n",
    "        bins.append(cut_point) \n",
    "    print(f'{feature} with equal width discretization => width = {width}')\n",
    "    print(bins)\n",
    "    print(\"=========================================================\")\n",
    "    # 儲存 discretization 後的值\n",
    "    bin_result = []\n",
    "    for value in att_value:\n",
    "        value = float(value)\n",
    "        for i, cut_value in enumerate(bins):\n",
    "            if i == 0 and value < cut_value:\n",
    "                bin_result.append(i + 1)\n",
    "                break\n",
    "            elif i == 0 and value > cut_value and value <= bins[i + 1]:\n",
    "                bin_result.append(i + 2)\n",
    "                break\n",
    "            elif i == bin_num - 2 and value > cut_value:\n",
    "                bin_result.append(i + 2)\n",
    "            elif value > cut_value and value <= bins[i + 1]:\n",
    "                bin_result.append(i + 2)\n",
    "                break\n",
    "    return bin_result\n",
    "\n",
    "def discretize_equal_width(df, features, bin_num):\n",
    "    new_df = []\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        bin_results[feature] = equal_width(df, feature, bin_num)\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "\n",
    "# 對原始資料所有連續型變數做離散化並存到新的 dict of list 裡\n",
    "equal_width_df = discretize_equal_width(df, X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection:\n",
      "Pass1: best_feature_subset = ['Ba'] , Goodness = 0.3042912813744425\n",
      "Forward Selection:\n",
      "Pass2: best_feature_subset = ['Ba', 'Mg'] , Goodness = 0.39168312637244257\n",
      "Forward Selection:\n",
      "Pass3: best_feature_subset = ['Ba', 'Mg', 'Ca'] , Goodness = 0.3973212810619197\n",
      "Forward Selection:\n",
      "Pass4: best_feature_subset = ['Ba', 'Mg', 'Ca', 'Na'] , Goodness = 0.405125062147965\n",
      "Forward Selection:\n",
      "Pass5: best_feature_subset = ['Ba', 'Mg', 'Ca', 'Na', 'Al'] , Goodness = 0.4111914788502334\n",
      "Forward Selection:\n",
      "Final select features: ['Ba', 'Mg', 'Ca', 'Na', 'Al'], Goodness = 0.4111914788502334\n",
      "=================================================================================================\n",
      "Backward Selection:\n",
      "Pass1: best_feature_subset = ['Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.3929712216460038\n",
      "remove feature: RI\n",
      "Backward Selection:\n",
      "Pass2: best_feature_subset = ['Na', 'Mg', 'Al', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.40295299406768176\n",
      "remove feature: Si\n",
      "Backward Selection:\n",
      "Pass3: best_feature_subset = ['Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'] , Goodness = 0.4094585798175886\n",
      "remove feature: Fe\n",
      "Backward Selection:\n",
      "Pass4: best_feature_subset = ['Na', 'Mg', 'Al', 'Ca', 'Ba'] , Goodness = 0.4111914788502334\n",
      "remove feature: K\n",
      "Backward Selection:\n",
      "Final select features: ['Na', 'Mg', 'Al', 'Ca', 'Ba'], Goodness = 0.4111914788502334\n"
     ]
    }
   ],
   "source": [
    "# 對所有特徵離散化後的資料集做 forward selection、backward selection\n",
    "forward_selection(equal_width_df, X, 'class')\n",
    "print(\"=================================================================================================\")\n",
    "backward_selection(equal_width_df, X, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with equal frequency discretization:\n",
      "[1.5159, 1.51629, 1.51667, 1.51732, 1.51766, 1.51808, 1.51852, 1.51977, 1.52177]\n",
      "=========================================================\n",
      "Na with equal frequency discretization:\n",
      "[12.67, 12.85, 13.0, 13.2, 13.33, 13.49, 13.73, 14.14, 14.56]\n",
      "=========================================================\n",
      "Mg with equal frequency discretization:\n",
      "[0.0, 2.72, 3.36, 3.48, 3.54, 3.59, 3.66, 3.82]\n",
      "=========================================================\n",
      "Al with equal frequency discretization:\n",
      "[0.83, 1.14, 1.23, 1.3, 1.38, 1.51, 1.58, 1.8, 2.12]\n",
      "=========================================================\n",
      "Si with equal frequency discretization:\n",
      "[71.77, 72.12, 72.38, 72.65, 72.78, 72.89, 73.01, 73.11, 73.28]\n",
      "=========================================================\n",
      "K with equal frequency discretization:\n",
      "[0.0, 0.11, 0.33, 0.52, 0.56, 0.58, 0.61, 0.66, 1.41]\n",
      "=========================================================\n",
      "Ca with equal frequency discretization:\n",
      "[7.96, 8.11, 8.32, 8.44, 8.6, 8.78, 9.02, 9.57, 10.88]\n",
      "=========================================================\n",
      "Ba with equal frequency discretization:\n",
      "[0.0, 0.76]\n",
      "=========================================================\n",
      "Fe with equal frequency discretization:\n",
      "[0.0, 0.11, 0.19, 0.32]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def equal_frequency(df,feature,bin_num):\n",
    "    # 先記錄每個 instance 的原始索引及 value，len(df[feature]) 為資料總筆數\n",
    "    att_value = [(i, float(df[feature][i])) for i in range(len(df[feature]))]\n",
    "    # 以 attribute value 的值排序\n",
    "    att_value.sort(key = lambda x : x[1])\n",
    "    # 計算每個 bin 應該包含的 instances 數量\n",
    "    frequency = len(att_value) // bin_num\n",
    "    bins = [] \n",
    "    start = att_value[0][1]\n",
    "    bin_index = 1    # 記錄目前 bin \n",
    "    cur_bin_cnt = 0  # 記錄目前 bin 所分配到的 value 個數\n",
    "    # att_value 已排序過，故會由小到大遍歷，org_index 是紀錄該筆 instance 在未排序前的位置\n",
    "    for cur,(org_index,value) in enumerate(att_value):\n",
    "        # 目前 bin 的 value 數量已滿足一個 bin 所應該分配到的 frequency\n",
    "        if cur_bin_cnt >= frequency and bin_index <= bin_num:\n",
    "            # 且當前 value 不等於前一個 value 值\n",
    "            if cur < len(att_value) and value != att_value[cur - 1][1]:\n",
    "                # 若已計算到最後一個 bin\n",
    "                if bin_index == bin_num:\n",
    "                    bins.append((start,att_value[-1][1]))  # end 即為最後一筆 instance(最大值)\n",
    "                    break\n",
    "                end = att_value[cur - 1][1]  # 該 bin 的區間最大值(不包含)\n",
    "                bins.append((start,end))\n",
    "\n",
    "                # 切換到下一個 bin\n",
    "                bin_index += 1\n",
    "                cur_bin_cnt = 0\n",
    "                start = att_value[cur - 1][1]  # att_value[i][1] 為下個 bin 的起點\n",
    "        # 該 bin 裡的 instances 個數加一\n",
    "        cur_bin_cnt += 1\n",
    "\n",
    "    # 上述設定在切換下個 bin 時才將 (start,end) 進 bins\n",
    "    # 有可能迴圈結束，最後一個 bin 的值個數不足一個 frequency，不會切換 bin，因此需要額外判斷防止最後一個 bin 消失\n",
    "    if len(bins) < bin_num:\n",
    "        end = att_value[-1][1]   # end 為最後一個元素(最大值)\n",
    "        bins.append((start, end))\n",
    "    print(f'{feature} with equal frequency discretization:')\n",
    "    print_bins = []\n",
    "    for i, (start, end) in enumerate(bins):\n",
    "        if i == len(bins) - 1:\n",
    "            break\n",
    "        print_bins.append(end)\n",
    "    print(print_bins)\n",
    "    print(\"=========================================================\")\n",
    "    # 對 df 做離散化並將新值存到一個 dict of list\n",
    "    org_value = df[feature]\n",
    "    bin_result = []\n",
    "    for value in org_value:\n",
    "        value = float(value)\n",
    "        for i, (start, end) in enumerate(bins):\n",
    "            if i == 0 and value >= start and value <= end:\n",
    "                bin_result.append(i + 1)\n",
    "                break\n",
    "            elif value > start and value <= end:\n",
    "                bin_result.append(i + 1)\n",
    "                break\n",
    "    return bin_result\n",
    "\n",
    "\n",
    "def discretize_equal_frequency(df, features, bin_num):\n",
    "    new_df = []\n",
    "    # 對每個 feature 做離散化，回傳 bin 結果(dict of list）\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        bin_results[feature] = equal_frequency(df, feature, bin_num)\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "\n",
    "equal_frequency_df = discretize_equal_frequency(df,X,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection:\n",
      "Pass1: best_feature_subset = ['Mg'] , Goodness = 0.24851240850960837\n",
      "Forward Selection:\n",
      "Pass2: best_feature_subset = ['Mg', 'Ba'] , Goodness = 0.3223383424643906\n",
      "Forward Selection:\n",
      "Pass3: best_feature_subset = ['Mg', 'Ba', 'Al'] , Goodness = 0.3611469799776129\n",
      "Forward Selection:\n",
      "Pass4: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI'] , Goodness = 0.3833761038884968\n",
      "Forward Selection:\n",
      "Pass5: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI', 'K'] , Goodness = 0.3960303164266635\n",
      "Forward Selection:\n",
      "Pass6: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI', 'K', 'Ca'] , Goodness = 0.39697499942233605\n",
      "Forward Selection:\n",
      "Pass7: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI', 'K', 'Ca', 'Na'] , Goodness = 0.3973256378646354\n",
      "Forward Selection:\n",
      "Final select features: ['Mg', 'Ba', 'Al', 'RI', 'K', 'Ca', 'Na'], Goodness = 0.3973256378646354\n",
      "=================================================================================================\n",
      "Backward Selection:\n",
      "Pass1: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.39160854990249794\n",
      "remove feature: Si\n",
      "Backward Selection:\n",
      "Pass2: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'] , Goodness = 0.3973256378646354\n",
      "remove feature: Fe\n",
      "Backward Selection:\n",
      "Final select features: ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'], Goodness = 0.3973256378646354\n"
     ]
    }
   ],
   "source": [
    "# 對所有特徵離散化後的資料集做 forward selection、backward selection\n",
    "forward_selection(equal_frequency_df, X, 'class')\n",
    "print(\"=================================================================================================\")\n",
    "backward_selection(equal_frequency_df, X, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算區間內類別值的 entropy\n",
    "def Ent(df,class_label):\n",
    "    class_value = df[class_label]\n",
    "    cnt = Counter(class_value)\n",
    "    prob = [count / len(class_value) for i,count in cnt.items()]\n",
    "    Ent = -sum(p * log2(p) for p in prob) \n",
    "    return Ent\n",
    "\n",
    "# cut_index 為資料被分割成兩個 subset 時，「右側區間的起始 index」，此函式計算切割後的資訊增益\n",
    "def info_gain(df, midpoint, feature, class_label):\n",
    "    total = len(df[class_label])\n",
    "\n",
    "    # 依據 feature 值是否 <= midpoint 來分割資料\n",
    "    left = {class_label: [df[class_label][i] for i in range(len(df[feature])) if float(df[feature][i]) <= midpoint]}\n",
    "    right = {class_label: [df[class_label][i] for i in range(len(df[feature])) if float(df[feature][i]) > midpoint]}\n",
    "\n",
    "    # 若任一邊為空，代表不是有效切點，資訊增益為 0\n",
    "    if not left[class_label] or not right[class_label]:\n",
    "        return 0\n",
    "\n",
    "    Ent_cut = (len(left[class_label]) / total) * Ent(left, class_label) + (len(right[class_label]) / total) * Ent(right, class_label)\n",
    "\n",
    "    return Ent(df, class_label) - Ent_cut\n",
    "\n",
    "# 找 feature 的最佳切點\n",
    "def find_cut_point(df, feature, class_label):\n",
    "    best_info_gain = -1\n",
    "    best_cut_value = None\n",
    "\n",
    "    # 整理為 (value, label) 配對並排序\n",
    "    value_label = [(float(df[feature][i]), df[class_label][i]) for i in range(len(df[feature]))]\n",
    "    value_label = sorted(value_label, key=lambda x: x[0])\n",
    "\n",
    "    for i in range(1, len(value_label)):\n",
    "        if value_label[i][1] != value_label[i - 1][1]:  # 類別不同才考慮切點\n",
    "            midpoint = (value_label[i][0] + value_label[i - 1][0]) / 2\n",
    "            cur_info_gain = info_gain(df, midpoint, feature, class_label)\n",
    "            if cur_info_gain > best_info_gain:\n",
    "                best_info_gain = cur_info_gain\n",
    "                best_cut_value = midpoint\n",
    "\n",
    "    return best_info_gain, best_cut_value\n",
    "\n",
    "# 對整個 df 的 feature 欄位做 entropy_base 切割，找所有切點，返回切割點 list\n",
    "def split(df,feature,class_label,cut_points):\n",
    "    # 若傳進來的 cut_points 為空，代表區間無可用的切割點\n",
    "    if cut_points is None:\n",
    "        cut_points = []\n",
    "    best_info_gain,best_cut_value = find_cut_point(df,feature,class_label)\n",
    "\n",
    "    # 若區間的 class 值或 feature 值都是一樣的，或只剩一個 instance，代表切割無意義\n",
    "    if (len(df[class_label]) <= 1 or best_cut_value is None or\n",
    "        len(set(df[class_label])) == 1 or len(set(df[feature])) == 1):\n",
    "        return None\n",
    "    \n",
    "    # 創造兩個 dict of list 儲存切割後的 feature 與 class 欄位\n",
    "    left_set = {feature : [],class_label : []}\n",
    "    right_set = {feature : [],class_label : []}\n",
    "    for i in range(len(df[feature])):\n",
    "        # 分配 instances 至對應的區間\n",
    "        value = float(df[feature][i])\n",
    "        if value <= best_cut_value:\n",
    "            left_set[feature].append(value)\n",
    "            left_set[class_label].append(df[class_label][i])\n",
    "        else:\n",
    "            right_set[feature].append(value)\n",
    "            right_set[class_label].append(df[class_label][i])\n",
    "    # 切割完，若任一區間沒有資料，或分割後區間內容與分割前一樣，代表分割沒有幫助 \n",
    "    if (len(left_set[feature]) == 0 or len(right_set[feature]) == 0 or\n",
    "        len(left_set[feature]) == len(df[feature]) or len(right_set[feature]) == len(df[feature])):\n",
    "        return None\n",
    "    # 計算 MDLPC criterion 的 threshold\n",
    "\n",
    "    # 計算初始區間、切割後的左右區間個包含的 class 種類數量\n",
    "    k = len(set(df[class_label]))\n",
    "    k1 = len(set(left_set[class_label]))\n",
    "    k2 = len(set(right_set[class_label]))\n",
    "    N = len(df[feature])\n",
    "    H_S = Ent(df, class_label)\n",
    "    H_l = Ent(left_set, class_label)\n",
    "    H_r = Ent(right_set, class_label)\n",
    "    delta = log2(3 ** k - 2) - (k * H_S - k1 * H_l - k2 * H_r)\n",
    "    threshold = log2(N - 1) / N + delta / N\n",
    "\n",
    "    # 最好的切割點的 gain 未超過 threshold，停止\n",
    "    if best_info_gain <= threshold :\n",
    "        return []\n",
    "    \n",
    "    cut_points.append(best_cut_value)\n",
    "    # 對左右區間遞迴做 entropy_base 切割\n",
    "    split(left_set,feature,class_label,cut_points)\n",
    "    split(right_set,feature,class_label,cut_points)\n",
    "    return cut_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with entropy base discretization:\n",
      "[1.517335, 1.517985]\n",
      "=========================================================\n",
      "Na with entropy base discretization:\n",
      "[14.065]\n",
      "=========================================================\n",
      "Mg with entropy base discretization:\n",
      "[2.6950000000000003]\n",
      "=========================================================\n",
      "Al with entropy base discretization:\n",
      "[1.38, 1.76]\n",
      "=========================================================\n",
      "Si with entropy base discretization:\n",
      "[]\n",
      "=========================================================\n",
      "K with entropy base discretization:\n",
      "[0.055, 0.61, 0.745]\n",
      "=========================================================\n",
      "Ca with entropy base discretization:\n",
      "[7.02, 8.28, 10.075]\n",
      "=========================================================\n",
      "Ba with entropy base discretization:\n",
      "[0.335]\n",
      "=========================================================\n",
      "Fe with entropy base discretization:\n",
      "[]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# 照上述的切點做 discretization\n",
    "def discretize_entropy_base(df,features,class_label):\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        print(f'{feature} with entropy base discretization:')    \n",
    "        cut_points = []\n",
    "        cut_points = split(df,feature,\"class\",cut_points)\n",
    "        # 若沒有切點，代表整個特徵內容會被離散為同個類別\n",
    "        if cut_points == []:\n",
    "            ent_base_res = [1] * len(df[feature])\n",
    "            bin_results[feature] = ent_base_res\n",
    "        else:\n",
    "            cut_points = sorted(cut_points)\n",
    "            org_value = df[feature]     # 紀錄特徵值的原始值\n",
    "            ent_base_res = []           # 紀錄離散化後的特徵值\n",
    "            # 對每個 feature 值做離散化分配\n",
    "            for value in org_value:\n",
    "                value = float(value)\n",
    "                for i,cut_point in enumerate(cut_points):\n",
    "                    # 只有一個切割點，只會被切成兩個區間\n",
    "                    if len(cut_points) == 1:\n",
    "                        if value <= cut_point:\n",
    "                            ent_base_res.append(i + 1)\n",
    "                        else:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                    # 有兩個以上的切點\n",
    "                    else:\n",
    "                        if i == 0 and value <= cut_point:\n",
    "                            ent_base_res.append(i + 1)\n",
    "                        elif i == 0 and value > cut_point and value <= cut_points[i + 1]:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                        elif i == len(cut_points) - 1 and value > cut_point:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                        elif value > cut_point and value <= cut_points[i + 1]:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                       \n",
    "            bin_results[feature] = ent_base_res\n",
    "        print(f\"{cut_points}\")\n",
    "        print(\"=========================================================\")\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "entropy_base_df = discretize_entropy_base(df,X,'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection:\n",
      "Pass1: best_feature_subset = ['Mg'] , Goodness = 0.37040112253047947\n",
      "Forward Selection:\n",
      "Pass2: best_feature_subset = ['Mg', 'Al'] , Goodness = 0.44111979963245035\n",
      "Forward Selection:\n",
      "Pass3: best_feature_subset = ['Mg', 'Al', 'Ca'] , Goodness = 0.4684573941293161\n",
      "Forward Selection:\n",
      "Pass4: best_feature_subset = ['Mg', 'Al', 'Ca', 'Ba'] , Goodness = 0.4923013144646698\n",
      "Forward Selection:\n",
      "Pass5: best_feature_subset = ['Mg', 'Al', 'Ca', 'Ba', 'K'] , Goodness = 0.5084334732749055\n",
      "Forward Selection:\n",
      "Final select features: ['Mg', 'Al', 'Ca', 'Ba', 'K'], Goodness = 0.5084334732749055\n",
      "=================================================================================================\n",
      "Backward Selection:\n",
      "Pass1: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.5106082375130458\n",
      "remove feature: Si\n",
      "Backward Selection:\n",
      "Pass2: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'] , Goodness = 0.5106082375130458\n",
      "remove feature: Fe\n",
      "Backward Selection:\n",
      "Final select features: ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'], Goodness = 0.5106082375130458\n"
     ]
    }
   ],
   "source": [
    "# 對所有特徵離散化後的資料集做 forward selection、backward selection\n",
    "forward_selection(entropy_base_df, X, 'class')\n",
    "print(\"=================================================================================================\")\n",
    "backward_selection(entropy_base_df, X, 'class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
