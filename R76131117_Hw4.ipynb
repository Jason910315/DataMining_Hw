{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "aacfe406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from math import sqrt\n",
    "from math import log2\n",
    "from collections import Counter,defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6cc66187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 txt 檔案\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "# 讀取資料\n",
    "data = load_data('glass.txt')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "29e3a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_column_dict(data, columns):\n",
    "    # 初始化一個空的 dictionary，key 是 attributes 名稱，value 是空 list：\n",
    "    df_dict = {col: [] for col in columns}\n",
    "    # 對每一列資料逐欄掃描，同時把欄位名稱 (col) 跟對應值 (val) 配對起來\n",
    "    for row in data:\n",
    "        for col, val in zip(columns, row):\n",
    "                val = float(val)\n",
    "                df_dict[col].append(val)\n",
    "    return df_dict\n",
    "# 定義欄位名稱\n",
    "columns = [\"Id\",\"RI\",\"Na\",\"Mg\",\"Al\",\"Si\",\"K\",\"Ca\",\"Ba\",\"Fe\",\"class\"]\n",
    "features = columns.copy()\n",
    "features.remove('Id')\n",
    "features.remove(\"class\")\n",
    "df = table_to_column_dict(data,columns)\n",
    "# 移除 Id 欄位\n",
    "del df[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9d5eb10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with equal width discretization => width = 0.0022780000000000022\n",
      "[1.513428, 1.515706, 1.517984, 1.520262, 1.52254, 1.524818, 1.527096, 1.529374, 1.531652]\n",
      "=========================================================\n",
      "Na with equal width discretization => width = 0.6649999999999998\n",
      "[11.395, 12.06, 12.725, 13.39, 14.055, 14.719999999999999, 15.384999999999998, 16.049999999999997, 16.715]\n",
      "=========================================================\n",
      "Mg with equal width discretization => width = 0.449\n",
      "[0.449, 0.898, 1.347, 1.796, 2.245, 2.694, 3.1430000000000002, 3.592, 4.041]\n",
      "=========================================================\n",
      "Al with equal width discretization => width = 0.321\n",
      "[0.611, 0.9319999999999999, 1.2530000000000001, 1.574, 1.895, 2.216, 2.537, 2.858, 3.1790000000000003]\n",
      "=========================================================\n",
      "Si with equal width discretization => width = 0.5599999999999994\n",
      "[70.37, 70.93, 71.49, 72.05, 72.61, 73.17, 73.73, 74.28999999999999, 74.85]\n",
      "=========================================================\n",
      "K with equal width discretization => width = 0.621\n",
      "[0.621, 1.242, 1.863, 2.484, 3.105, 3.726, 4.3469999999999995, 4.968, 5.589]\n",
      "=========================================================\n",
      "Ca with equal width discretization => width = 1.076\n",
      "[6.506, 7.582, 8.658, 9.734, 10.81, 11.886, 12.962, 14.038, 15.114]\n",
      "=========================================================\n",
      "Ba with equal width discretization => width = 0.315\n",
      "[0.315, 0.63, 0.9450000000000001, 1.26, 1.575, 1.8900000000000001, 2.205, 2.52, 2.835]\n",
      "=========================================================\n",
      "Fe with equal width discretization => width = 0.051000000000000004\n",
      "[0.051000000000000004, 0.10200000000000001, 0.15300000000000002, 0.20400000000000001, 0.255, 0.30600000000000005, 0.35700000000000004, 0.40800000000000003, 0.459]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def equal_width(df, feature, bin_num):\n",
    "    # 計算每組 bin 區間\n",
    "    att_value = df[feature]\n",
    "    max_value = float(max(att_value))\n",
    "    min_value = float(min(att_value))\n",
    "    \n",
    "    # 計算每組區間寬度\n",
    "    width = (max_value - min_value) / bin_num\n",
    "    \n",
    "    # 計算切分點\n",
    "    bins = [min_value + i * width for i in range(1, bin_num)]\n",
    "    \n",
    "    print(f'{feature} with equal width discretization => width = {width}')\n",
    "    print(bins)\n",
    "    print(\"=========================================================\")\n",
    "    # 儲存 discretization 後的值\n",
    "    bin_result = []\n",
    "    for value in att_value:\n",
    "        value = float(value)\n",
    "        # 找到第一個大於 value 的切分點索引\n",
    "        bin_index = 0\n",
    "        for i, cut_value in enumerate(bins):\n",
    "            if value <= cut_value:\n",
    "                bin_index = i + 1\n",
    "                break\n",
    "            elif i == len(bins) - 1:  # 如果是最後一個切分點\n",
    "                bin_index = i + 2\n",
    "        bin_result.append(bin_index)\n",
    "    return bin_result\n",
    "def discretize_equal_width(df, features, bin_num):\n",
    "    new_df = []\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        bin_results[feature] = equal_width(df, feature, bin_num)\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "\n",
    "# 對原始資料所有連續型變數做離散化並存到新的 dict of list 裡\n",
    "\n",
    "equal_width_df = discretize_equal_width(df, features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "69aa9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model with 20 base models:\n",
      "Fold 1 => accuracy = 20/43 = 0.46511627906976744\n",
      "Fold 2 => accuracy = 30/43 = 0.6976744186046512\n",
      "Fold 3 => accuracy = 30/43 = 0.6976744186046512\n",
      "Fold 4 => accuracy = 20/43 = 0.46511627906976744\n",
      "Fold 5 => accuracy = 21/42 = 0.5\n",
      "0.5651162790697675\n"
     ]
    }
   ],
   "source": [
    "# 建立 naive bayse 模型\n",
    "def NaiveBayes(df, training_set, class_name):\n",
    "    # 計算每個類別的先驗機率 P(class_name = C_j)\n",
    "    N = len(training_set[class_name])\n",
    "    class_values = list(set(df[class_name]))\n",
    "    prior_C = {c : training_set[class_name].count(c) / N for c in class_values}\n",
    "    features = list(df.keys())\n",
    "    features.remove(class_name)\n",
    "    # 建立一個巢狀的 defaultdict 來儲存條件機率 P(Xi|Cj)\n",
    "    # 第一層 key 是類別 Cj\n",
    "    # 第二層 key 是特徵 Xi \n",
    "    # 第三層 key 是特徵值，value 是該特徵值在類別 Cj 下的條件機率\n",
    "    p_Xi_Cj_dict = defaultdict(lambda: defaultdict(dict))\n",
    "    # 取得所有特徵的可能的特徵值\n",
    "    possible_Xi_values = {Xi: set(df[Xi]) for Xi in features}\n",
    "    for Cj in class_values:\n",
    "        # 取出 training_set 中類別 == Cj 的樣本的位址\n",
    "        \n",
    "        Cj_index = []\n",
    "        for i in range(len(training_set[class_name])):  \n",
    "            if training_set[class_name][i] == Cj:\n",
    "                Cj_index.append(i)\n",
    "       \n",
    "        for Xi in features:\n",
    "            # 計算類別為 Cj 的訓練資料中，Xi 特徵可能出現的值的個數\n",
    "            Xi_value_count = Counter([training_set[Xi][i] for i in Cj_index])\n",
    "\n",
    "            # 計算 Xi 特徵每種可能的值出現在 Cj 類別的機率 (包括 training_set 未出現過的可能值)\n",
    "            for value in possible_Xi_values[Xi]:\n",
    "                # 若 value 不存在則預設為 0\n",
    "                count = Xi_value_count.get(value, 0)\n",
    "                # 做 laplace 平滑化\n",
    "                p_Xi_Cj_dict[Cj][Xi][value] = (count + 1) / (len(Cj_index) + 10)\n",
    "    return prior_C, p_Xi_Cj_dict\n",
    "\n",
    "def ensemble_model_with_bagging(df, m, training_set, test_set, class_name):\n",
    "    N = len(training_set[class_name])\n",
    "    # 儲存 training_set 所計算出的 P(Xi | Cj) 機率\n",
    "    p_Xi_Cj_dict = defaultdict(lambda: defaultdict(dict))\n",
    "    predictions = []\n",
    "    # 集成 m 個 base models 的預測結果  \n",
    "    for _ in range(m):\n",
    "        # 生成 0 ~ N-1 範圍內的隨機亂數，總共生成 N 個\n",
    "        # 這步驟代表 bagging 的取後放回抽樣，陣列裡的每個元素即為抽到的訓練集樣本索引\n",
    "        sampled_indices = random.choices(range(N), k = N)\n",
    "     \n",
    "        # 每次的 bagging sample 會成為每個 base model 所使用到的訓練集 \n",
    "        bagging_sample = {key: [training_set[key][i] for i in sampled_indices] for key in training_set.keys()}\n",
    "        # 計算該 base model 的先驗機率 P(Cj) 與 P(Xi | Cj)\n",
    "        prior_C, p_Xi_Cj_dict = NaiveBayes(df, bagging_sample, class_name)\n",
    "        features = list(df.keys())\n",
    "        features.remove(class_name)\n",
    "        class_values = list(set(df[class_name]))\n",
    "\n",
    "        # 預測 test_set\n",
    "        fold_predictions = []\n",
    "        for i in range(len(test_set[class_name])):\n",
    "            max_prob = 0\n",
    "            # 取出第 i 個 test_set 樣本的所有特徵的值\n",
    "            instance = {f : test_set[f][i] for f in features} \n",
    "            predict_j = None\n",
    "            # 計算該特徵值組合在 Cj 類別下發生機率\n",
    "            for Cj in class_values:\n",
    "                p_Xi_Cj = 1\n",
    "                for Xi in features:  \n",
    "                    p_Xi_Cj *=  p_Xi_Cj_dict[Cj][Xi][instance[Xi]]\n",
    "                 \n",
    "                posterior_prob = prior_C[Cj] * p_Xi_Cj\n",
    "                \n",
    "                if posterior_prob > max_prob:\n",
    "                    predict_j = Cj\n",
    "                    max_prob = posterior_prob\n",
    "            fold_predictions.append(predict_j)\n",
    "            \n",
    "        predictions.append(fold_predictions)\n",
    " \n",
    "    final_predictions = []\n",
    "    for i in range(len(test_set[class_name])):\n",
    "        # 取得每個樣本在所有base model的預測結果\n",
    "        sample_predictions = [pred[i] for pred in predictions]\n",
    "        # 選出最常見的預測結果\n",
    "        final_predictions.append(Counter(sample_predictions).most_common()[0][0])\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "def five_folds_cv(df, base, class_name):\n",
    "    n = 214\n",
    "    indices = list(range(n))\n",
    "    # 隨機打亂索引\n",
    "    rng = random.Random(49)\n",
    "    rng.shuffle(indices)\n",
    "    fold_sizes = [43,43,43,43,42]\n",
    "    # 儲存每個 folds 所包含的樣本索引值\n",
    "    folds = [[] for _ in range(5)]\n",
    "    \n",
    "    # 將打亂的 index  一一分配到 folds 中\n",
    "    current_index = 0\n",
    "    for fold_index, fold_size in enumerate(fold_sizes):\n",
    "        folds[fold_index] = indices[current_index : current_index + fold_size]\n",
    "        current_index += fold_size\n",
    "    # 對應到實際資料\n",
    "    fold_data = []  # 儲存 five-folds 中每個 fold 的資料\n",
    "    for fold_indice in folds:\n",
    "        fold = {key : [df[key][i] for i in fold_indice] for key in df.keys()}\n",
    "        fold_data.append(fold)\n",
    "    # 進行 5-folds 預測，all_prediction 儲存五次預測的結果\n",
    "    all_predictions = {}\n",
    "    for k in range(5):\n",
    "        # fold_data 是一個陣列，fold_data[k] 即代表第 k 個 fold 的資料\n",
    "        test_set = fold_data[k]\n",
    "        # k - 1 個 folds 合併成 training_set\n",
    "        training_set = {key: [] for key in df.keys()}\n",
    "        for i in range(5):\n",
    "            if i != k:\n",
    "                # 將其餘四個fold的個別特徵值合併\n",
    "                for key in df.keys():\n",
    "                    training_set[key].extend(fold_data[i][key])\n",
    "        # 用集成模型預測\n",
    "        predictions = ensemble_model_with_bagging(df, base, training_set, test_set, class_name)\n",
    "        all_predictions[k] = predictions\n",
    "\n",
    "    print(f\"Ensemble model with {base} base models:\")\n",
    "    avg = 0\n",
    "    # 計算每個 fold 的準確率\n",
    "    for k in range(5):\n",
    "        correct = sum(1 for pred, true in zip(all_predictions[k], fold_data[k][class_name]) if pred == true)\n",
    "        accuracy = correct / len(all_predictions[k])\n",
    "        print(f\"Fold {k + 1} => accuracy = {correct}/{len(all_predictions[k])} = {accuracy}\")\n",
    "        avg += accuracy\n",
    "    print(avg / 5)\n",
    "    return\n",
    "\n",
    "five_folds_cv(equal_width_df, 20 ,'class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
