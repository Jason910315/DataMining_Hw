{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from math import sqrt\n",
    "from math import log2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 txt 檔案\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "# 讀取資料\n",
    "data = load_data('glass.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_column_dict(data, columns):\n",
    "    # 初始化一個空的 dictionary，key 是 attributes 名稱，value 是空 list：\n",
    "    df_dict = {col: [] for col in columns}\n",
    "    # 對每一列資料逐欄掃描，同時把欄位名稱 (col) 跟對應值 (val) 配對起來\n",
    "    for row in data:\n",
    "        for col, val in zip(columns, row):\n",
    "                val = float(val)\n",
    "                df_dict[col].append(val)\n",
    "    return df_dict\n",
    "# 定義欄位名稱\n",
    "columns = [\"Id\",\"RI\",\"Na\",\"Mg\",\"Al\",\"Si\",\"K\",\"Ca\",\"Ba\",\"Fe\",\"class\"]\n",
    "df = table_to_column_dict(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = columns.copy()\n",
    "X.remove(\"Id\")\n",
    "X.remove(\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算 feature entropy\n",
    "def entropy(df,feature):  \n",
    "    att_value = df[feature]  # 取出 dict 的特定 attribute 的所有資料\n",
    "    value_count = Counter(att_value)          # 計算所有可能值的個數\n",
    "    total = len(att_value)\n",
    "    prob = [count / total for key,count in value_count.items()]  # 計算每個 attribute_value 的機率\n",
    "    return -sum(p * log2(p) for p in prob)\n",
    "\n",
    "# 計算特徵 X、Y 間的 Mutual Information\n",
    "def mutual_information(df,X, Y):\n",
    "    X_list = df[X]\n",
    "    Y_list = df[Y]\n",
    "    # 計算 X 和 Y 的熵\n",
    "    H_X = entropy(df ,X)\n",
    "    H_Y = entropy(df ,Y)\n",
    "    # 計算 X 和 Y 的聯合機率\n",
    "    joint_pairs = list(zip(X_list, Y_list))\n",
    "    joint_counts = Counter(joint_pairs)\n",
    "    total = len(X_list)\n",
    "    joint_prob = [count / total for key,count in joint_counts.items()]\n",
    "    H_X_Y = -sum(p * log2(p) for p in joint_prob)\n",
    "    return H_X + H_Y - H_X_Y\n",
    "    \n",
    "# 計算特徵 X、Y 的 symmetric uncertainty\n",
    "def cal_su(df,X,Y):\n",
    "    H_X = entropy(df,X)\n",
    "    H_Y = entropy(df,Y)\n",
    "    if H_X == 0 and H_Y == 0:\n",
    "        return 0\n",
    "    return 2 * (mutual_information(df,X,Y) / (H_X + H_Y))\n",
    "\n",
    "# 計算選取的特徵子集對於類別預測的 Goodness\n",
    "def Goodness(df,feature_subset,label):\n",
    "    su_X_C = 0\n",
    "    sum_su_X_Y = 0  \n",
    "    # 計算 feature_subset 內所有特徵對於類別值的 Symmetric uncertainty\n",
    "    su_X_C = sum(cal_su(df,X,label) for X in feature_subset)\n",
    "    \n",
    "    # 計算 feature_subset 內所有兩兩特徵間的 Symmetric uncertainty\n",
    "    for feature_i in feature_subset:\n",
    "        for feature_j in feature_subset:\n",
    "            sum_su_X_Y += cal_su(df,feature_i,feature_j)\n",
    "    if sum_su_X_Y == 0:\n",
    "        return 0\n",
    "    return su_X_C / sqrt(sum_su_X_Y)\n",
    "\n",
    "def forward_selection(df, X, y):\n",
    "    select_features = []    \n",
    "    best_score = 0.0        \n",
    "    remaining_features = X.copy()  \n",
    "    # 持續檢查直到沒有可以選擇的 feature\n",
    "    i = 1   \n",
    "    while(len(remaining_features) > 0):\n",
    "        scores = []  \n",
    "        for feature in remaining_features:\n",
    "            # temp_features 暫存此次循環的特徵組合 => 上回以選取好的最佳組合 select_features + 這回新選入的一個 feature\n",
    "            temp_features = select_features + [feature]\n",
    "            score = Goodness(df,temp_features,y)\n",
    "            # (目前的特徵組合, 新選進來的特徵, 此特徵組合的 Goodness)\n",
    "            scores.append((temp_features,feature,score))\n",
    "\n",
    "        # 依照 Goodness 排序\n",
    "        scores.sort(key=lambda x: x[2], reverse = True)  \n",
    "        best_new_score = float(scores[0][2])  \n",
    "        print(\"Forward Selection:\")\n",
    "        if(best_new_score > best_score):\n",
    "            best_score = best_new_score\n",
    "            select_features = scores[0][0]  # 更新成 Goodness 最優的 subset\n",
    "            if scores[0][1] in remaining_features:\n",
    "                remaining_features.remove(scores[0][1])  # 移除新選特徵\n",
    "            print(f\"Pass{i}: best_feature_subset = {select_features} , Goodness = {best_score}\")\n",
    "            i += 1\n",
    "        # 此輪中所有 feature_subset 的表現皆不如上一輪，Stop\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Final select features: {select_features}, Goodness = {best_score}\")\n",
    "\n",
    "def backward_selection(df, X, y):\n",
    "    select_features = X    \n",
    "    best_score = 0.0       \n",
    "    i = 1\n",
    "    # 持續檢查到選擇的 feature 只剩下一個\n",
    "    while(len(select_features) > 1):\n",
    "        scores = [] \n",
    "        for feature in select_features:\n",
    "            temp_features = select_features.copy()\n",
    "            # 每次移除一個 feature\n",
    "            temp_features.remove(feature)\n",
    "            score = Goodness(df,temp_features,y)\n",
    "            # (目前的特徵組合, 移除的特徵, 此特徵組合的 Goodness)\n",
    "            scores.append((temp_features,feature,score))\n",
    "\n",
    "        scores.sort(key = lambda x: x[2], reverse = True)  \n",
    "        best_new_score = float(scores[0][2]) \n",
    "        print(\"Backward Selection:\")\n",
    "        if(best_new_score >= best_score):\n",
    "            best_score = best_new_score\n",
    "            select_features = scores[0][0]  # 更新成 Goodness 最優的 subset\n",
    "            if scores[0][1] in select_features:\n",
    "                select_features.remove(scores[0][1])  # 移除特徵\n",
    "            print(f\"Pass{i}: best_feature_subset = {select_features} , Goodness = {best_score}\")\n",
    "            print(f\"remove feature: {scores[0][1]}\")\n",
    "            i += 1\n",
    "        # 此輪中所有 feature_subset 的表現皆不如上一輪，Stop\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Final select features: {select_features}, Goodness = {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with equal width discretization => width = 0.00228\n",
      "[1.513428, 1.515706, 1.517984, 1.520262, 1.52254, 1.524818, 1.527096, 1.529374, 1.531652]\n",
      "=========================================================\n",
      "Na with equal width discretization => width = 0.665\n",
      "[11.395, 12.06, 12.725, 13.39, 14.055, 14.719999999999999, 15.384999999999998, 16.049999999999997, 16.715]\n",
      "=========================================================\n",
      "Mg with equal width discretization => width = 0.449\n",
      "[0.449, 0.898, 1.347, 1.796, 2.245, 2.694, 3.1430000000000002, 3.592, 4.041]\n",
      "=========================================================\n",
      "Al with equal width discretization => width = 0.321\n",
      "[0.611, 0.9319999999999999, 1.2530000000000001, 1.574, 1.895, 2.216, 2.537, 2.858, 3.1790000000000003]\n",
      "=========================================================\n",
      "Si with equal width discretization => width = 0.56\n",
      "[70.37, 70.93, 71.49, 72.05, 72.61, 73.17, 73.73, 74.28999999999999, 74.85]\n",
      "=========================================================\n",
      "K with equal width discretization => width = 0.621\n",
      "[0.621, 1.242, 1.863, 2.484, 3.105, 3.726, 4.3469999999999995, 4.968, 5.589]\n",
      "=========================================================\n",
      "Ca with equal width discretization => width = 1.076\n",
      "[6.506, 7.582, 8.658, 9.734, 10.81, 11.886, 12.962, 14.038, 15.114]\n",
      "=========================================================\n",
      "Ba with equal width discretization => width = 0.315\n",
      "[0.315, 0.63, 0.9450000000000001, 1.26, 1.575, 1.8900000000000001, 2.205, 2.52, 2.835]\n",
      "=========================================================\n",
      "Fe with equal width discretization => width = 0.051\n",
      "[0.051000000000000004, 0.10200000000000001, 0.15300000000000002, 0.20400000000000001, 0.255, 0.30600000000000005, 0.35700000000000004, 0.40800000000000003, 0.459]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def equal_width(df,feature,bin_num):\n",
    "    # 計算每組 bin 區間\n",
    "    att_value = df[feature]\n",
    "    max_value = float(max(att_value))\n",
    "    min_value = float(min(att_value))\n",
    "    # 計算每組區間寬度\n",
    "    width = (max_value - min_value) / bin_num\n",
    "\n",
    "    bins = []  # 儲存每組區間範圍\n",
    "    # 計算每組區間數值範圍\n",
    "    for i in range(1, bin_num):\n",
    "        cut_point = min_value + i * width  # 取到小數點第5位\n",
    "        bins.append(cut_point)\n",
    "\n",
    "    print(f'{feature} with equal width discretization => width = {round(width, 5)}')\n",
    "    print(bins)\n",
    "    print(\"=========================================================\")\n",
    "    # 儲存 discretization 後的值\n",
    "    bin_result = []\n",
    "    for value in att_value:\n",
    "        value = float(value)\n",
    "        for i, cut_value in enumerate(bins):\n",
    "            if i == 0 and value <= cut_value:\n",
    "                bin_result.append(i + 1)\n",
    "                break\n",
    "            elif i == 0 and value > cut_value and value <= bins[i + 1]:\n",
    "                bin_result.append(i + 2)\n",
    "                break\n",
    "            elif i == bin_num - 2 and value > cut_value:\n",
    "                bin_result.append(i + 2)\n",
    "            elif i == bin_num - 2 and value <= cut_value and value > bin[i - 1]:\n",
    "                bin_result.append(i + 1)\n",
    "            elif value > cut_value and value <= bins[i + 1]:\n",
    "                bin_result.append(i + 2)\n",
    "                break\n",
    "    return bin_result\n",
    "\n",
    "def discretize_equal_width(df, features, bin_num):\n",
    "    new_df = []\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        bin_results[feature] = equal_width(df, feature, bin_num)\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "\n",
    "# 對原始資料所有連續型變數做離散化並存到新的 dict of list 裡\n",
    "equal_width_df = discretize_equal_width(df, X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection:\n",
      "Pass1: best_feature_subset = ['Ba'] , Goodness = 0.3042912813744425\n",
      "Forward Selection:\n",
      "Pass2: best_feature_subset = ['Ba', 'Mg'] , Goodness = 0.39168312637244257\n",
      "Forward Selection:\n",
      "Pass3: best_feature_subset = ['Ba', 'Mg', 'Ca'] , Goodness = 0.3973212810619197\n",
      "Forward Selection:\n",
      "Pass4: best_feature_subset = ['Ba', 'Mg', 'Ca', 'Na'] , Goodness = 0.405125062147965\n",
      "Forward Selection:\n",
      "Pass5: best_feature_subset = ['Ba', 'Mg', 'Ca', 'Na', 'Al'] , Goodness = 0.4111914788502334\n",
      "Forward Selection:\n",
      "Final select features: ['Ba', 'Mg', 'Ca', 'Na', 'Al'], Goodness = 0.4111914788502334\n",
      "=================================================================================================\n",
      "Backward Selection:\n",
      "Pass1: best_feature_subset = ['Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.3929712216460038\n",
      "remove feature: RI\n",
      "Backward Selection:\n",
      "Pass2: best_feature_subset = ['Na', 'Mg', 'Al', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.40295299406768176\n",
      "remove feature: Si\n",
      "Backward Selection:\n",
      "Pass3: best_feature_subset = ['Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'] , Goodness = 0.4094585798175886\n",
      "remove feature: Fe\n",
      "Backward Selection:\n",
      "Pass4: best_feature_subset = ['Na', 'Mg', 'Al', 'Ca', 'Ba'] , Goodness = 0.4111914788502334\n",
      "remove feature: K\n",
      "Backward Selection:\n",
      "Final select features: ['Na', 'Mg', 'Al', 'Ca', 'Ba'], Goodness = 0.4111914788502334\n"
     ]
    }
   ],
   "source": [
    "# 對所有特徵離散化後的資料集做 forward selection、backward selection\n",
    "forward_selection(equal_width_df, X, 'class')\n",
    "print(\"=================================================================================================\")\n",
    "backward_selection(equal_width_df, X, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with equal frequency discretization:\n",
      "[1.5159, 1.51629, 1.51667, 1.51732, 1.51766, 1.51808, 1.51852, 1.51977, 1.52177]\n",
      "=========================================================\n",
      "Na with equal frequency discretization:\n",
      "[12.67, 12.85, 13.0, 13.2, 13.33, 13.49, 13.73, 14.14, 14.56]\n",
      "=========================================================\n",
      "Mg with equal frequency discretization:\n",
      "[0.0, 2.72, 3.36, 3.48, 3.54, 3.59, 3.66, 3.82]\n",
      "=========================================================\n",
      "Al with equal frequency discretization:\n",
      "[0.83, 1.14, 1.23, 1.3, 1.38, 1.51, 1.58, 1.8, 2.12]\n",
      "=========================================================\n",
      "Si with equal frequency discretization:\n",
      "[71.77, 72.12, 72.38, 72.65, 72.78, 72.89, 73.01, 73.11, 73.28]\n",
      "=========================================================\n",
      "K with equal frequency discretization:\n",
      "[0.0, 0.11, 0.33, 0.52, 0.56, 0.58, 0.61, 0.66, 1.41]\n",
      "=========================================================\n",
      "Ca with equal frequency discretization:\n",
      "[7.96, 8.11, 8.32, 8.44, 8.6, 8.78, 9.02, 9.57, 10.88]\n",
      "=========================================================\n",
      "Ba with equal frequency discretization:\n",
      "[0.0, 0.76]\n",
      "=========================================================\n",
      "Fe with equal frequency discretization:\n",
      "[0.0, 0.11, 0.19, 0.32]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def equal_frequency(df,feature,bin_num):\n",
    "    # 先記錄每個 instance 的原始索引及 value，len(df[feature]) 為資料總筆數\n",
    "    att_value = [(i, float(df[feature][i])) for i in range(len(df[feature]))]\n",
    "    # 以 attribute value 的值排序\n",
    "    att_value.sort(key = lambda x : x[1])\n",
    "    # 計算每個 bin 應該包含的 instances 數量\n",
    "    frequency = len(att_value) // bin_num\n",
    "    bins = [] \n",
    "    start = att_value[0][1]\n",
    "    bin_index = 1    # 記錄目前 bin \n",
    "    cur_bin_cnt = 0  # 記錄目前 bin 所分配到的 value 個數\n",
    "    # att_value 已排序過，故會由小到大遍歷，org_index 是紀錄該筆 instance 在未排序前的位置\n",
    "    for cur,(org_index,value) in enumerate(att_value):\n",
    "        # 目前 bin 的 value 數量已滿足一個 bin 所應該分配到的 frequency\n",
    "        if cur_bin_cnt >= frequency and bin_index <= bin_num:\n",
    "            # 且當前 value 不等於前一個 value 值\n",
    "            if cur < len(att_value) and value != att_value[cur - 1][1]:\n",
    "                # 若已計算到最後一個 bin\n",
    "                if bin_index == bin_num:\n",
    "                    bins.append((start,att_value[-1][1]))  # end 即為最後一筆 instance(最大值)\n",
    "                    break\n",
    "                end = att_value[cur - 1][1]  # 該 bin 的區間最大值(包含)\n",
    "                bins.append((start,end))\n",
    "\n",
    "                # 切換到下一個 bin\n",
    "                bin_index += 1\n",
    "                cur_bin_cnt = 0\n",
    "                start = value  # att_value[i][1] 為下個 bin 的起點\n",
    "        # 該 bin 裡的 instances 個數加一\n",
    "        cur_bin_cnt += 1\n",
    "\n",
    "    # 上述設定在切換下個 bin 時才將 (start,end) 進 bins\n",
    "    # 有可能迴圈結束，最後一個 bin 的值個數不足一個 frequency，不會切換 bin，因此需要額外判斷防止最後一個 bin 消失\n",
    "    if len(bins) < bin_num:\n",
    "        end = att_value[-1][1]   # end 為最後一個元素(最大值)\n",
    "        bins.append((start, end))\n",
    "    print(f'{feature} with equal frequency discretization:')\n",
    "    print_bins = []\n",
    "    for i, (start, end) in enumerate(bins):\n",
    "        if i == len(bins) - 1:\n",
    "            break\n",
    "        print_bins.append(end)\n",
    "    print(print_bins)\n",
    "    print(\"=========================================================\")\n",
    "    # 對 df 做離散化並將新值存到一個 dict of list\n",
    "    org_value = df[feature]\n",
    "    bin_result = []\n",
    "    for value in org_value:\n",
    "        value = float(value)\n",
    "        for i, (start, end) in enumerate(bins):\n",
    "            if i == 0 and value >= start and value <= end:\n",
    "                bin_result.append(i + 1)\n",
    "                break\n",
    "            elif value >= start and value <= end:\n",
    "                bin_result.append(i + 1)\n",
    "                break\n",
    "    return bin_result\n",
    "\n",
    "\n",
    "def discretize_equal_frequency(df, features, bin_num):\n",
    "    new_df = []\n",
    "    # 對每個 feature 做離散化，回傳 bin 結果(dict of list）\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        bin_results[feature] = equal_frequency(df, feature, bin_num)\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "\n",
    "equal_frequency_df = discretize_equal_frequency(df,X,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection:\n",
      "Pass1: best_feature_subset = ['Mg'] , Goodness = 0.24851240850960837\n",
      "Forward Selection:\n",
      "Pass2: best_feature_subset = ['Mg', 'Ba'] , Goodness = 0.3223383424643906\n",
      "Forward Selection:\n",
      "Pass3: best_feature_subset = ['Mg', 'Ba', 'Al'] , Goodness = 0.3611469799776129\n",
      "Forward Selection:\n",
      "Pass4: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI'] , Goodness = 0.3833761038884968\n",
      "Forward Selection:\n",
      "Pass5: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI', 'K'] , Goodness = 0.3960303164266635\n",
      "Forward Selection:\n",
      "Pass6: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI', 'K', 'Ca'] , Goodness = 0.39697499942233605\n",
      "Forward Selection:\n",
      "Pass7: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI', 'K', 'Ca', 'Na'] , Goodness = 0.3973256378646354\n",
      "Forward Selection:\n",
      "Final select features: ['Mg', 'Ba', 'Al', 'RI', 'K', 'Ca', 'Na'], Goodness = 0.3973256378646354\n",
      "=================================================================================================\n",
      "Backward Selection:\n",
      "Pass1: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.39160854990249794\n",
      "remove feature: Si\n",
      "Backward Selection:\n",
      "Pass2: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'] , Goodness = 0.3973256378646354\n",
      "remove feature: Fe\n",
      "Backward Selection:\n",
      "Final select features: ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'], Goodness = 0.3973256378646354\n"
     ]
    }
   ],
   "source": [
    "# 對所有特徵離散化後的資料集做 forward selection、backward selection\n",
    "forward_selection(equal_frequency_df, X, 'class')\n",
    "print(\"=================================================================================================\")\n",
    "backward_selection(equal_frequency_df, X, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算區間內類別值的 entropy\n",
    "def Ent(df,class_label):\n",
    "    class_value = df[class_label]\n",
    "    cnt = Counter(class_value)\n",
    "    prob = [count / len(class_value) for i,count in cnt.items()]\n",
    "    Ent = -sum(p * log2(p) for p in prob) \n",
    "    return Ent\n",
    "\n",
    "# cut_index 為資料被分割成兩個 subset 時，「右側區間的起始 index」，此函式計算切割後的資訊增益\n",
    "def info_gain(df, cutpoint, feature, class_label):\n",
    "    total = len(df[class_label])\n",
    "\n",
    "    # 依據 feature 值是否 <= midpoint 來分割資料\n",
    "    left = {class_label: [df[class_label][i] for i in range(len(df[feature])) if float(df[feature][i]) <= cutpoint]}\n",
    "    right = {class_label: [df[class_label][i] for i in range(len(df[feature])) if float(df[feature][i]) > cutpoint]}\n",
    "\n",
    "    # 若任一邊為空，代表不是有效切點，資訊增益為 0\n",
    "    if not left[class_label] or not right[class_label]:\n",
    "        return 0\n",
    "\n",
    "    Ent_cut = (len(left[class_label]) / total) * Ent(left, class_label) + (len(right[class_label]) / total) * Ent(right, class_label)\n",
    "\n",
    "    return Ent(df, class_label) - Ent_cut\n",
    "\n",
    "# 找 feature 的最佳切點\n",
    "def find_cut_point(df, feature, class_label):\n",
    "    best_info_gain = -1\n",
    "    best_cut_value = None\n",
    "\n",
    "    # 整理為 (value, label) 配對並排序\n",
    "    value_label = sorted([(float(df[feature][i]), df[class_label][i], i) for i in range(len(df[feature]))], key=lambda x: (x[0],x[2]))\n",
    "    for i in range(1, len(value_label)):\n",
    "        if value_label[i][0] != value_label[i - 1][0]:\n",
    "            if(i < len(value_label) - 1):\n",
    "                if value_label[i + 1][0] != value_label[i][0]:\n",
    "                    if value_label[i][1] != value_label[i - 1][1]:  # 類別不同才考慮切點\n",
    "                        midpoint = (value_label[i][0] + value_label[i - 1][0]) / 2\n",
    "                        cur_info_gain = info_gain(df, midpoint, feature, class_label)\n",
    "                        if cur_info_gain > best_info_gain:\n",
    "                            \n",
    "                            best_info_gain = cur_info_gain\n",
    "                            best_cut_value = midpoint\n",
    "                else:\n",
    "                    j = i\n",
    "                    while(j < len(value_label) - 1 and value_label[j][0] == value_label[j + 1][0]):\n",
    "                        if value_label[j][1] != value_label[i - 1][1]:  # 類別不同才考慮切點\n",
    "                            midpoint = (value_label[j][0] + value_label[i - 1][0]) / 2\n",
    "                            cur_info_gain = info_gain(df, midpoint, feature, class_label)\n",
    "                            if cur_info_gain > best_info_gain:\n",
    "                                best_info_gain = cur_info_gain\n",
    "                                best_cut_value = midpoint\n",
    "                        j += 1\n",
    "            \n",
    "\n",
    "    return best_info_gain, best_cut_value\n",
    "\n",
    "# 對整個 df 的 feature 欄位做 entropy_base 切割，找所有切點，返回切割點 list\n",
    "def split(df,feature,class_label,cut_points):\n",
    "    # 若傳進來的 cut_points 為空，代表區間無可用的切割點\n",
    "    if cut_points is None:\n",
    "        cut_points = []\n",
    "    best_info_gain,best_cut_value = find_cut_point(df,feature,class_label)\n",
    "\n",
    "    # 若區間的 class 值或 feature 值都是一樣的，或只剩一個 instance，代表切割無意義\n",
    "    if (len(df[class_label]) <= 1 or best_cut_value is None or\n",
    "        len(set(df[class_label])) == 1 or len(set(df[feature])) == 1):\n",
    "        return []\n",
    "    \n",
    "    # 創造兩個 dict of list 儲存切割後的 feature 與 class 欄位\n",
    "    left_set = {feature : [],class_label : []}\n",
    "    right_set = {feature : [],class_label : []}\n",
    "    for i in range(len(df[feature])):\n",
    "        # 分配 instances 至對應的區間\n",
    "        value = float(df[feature][i])\n",
    "        if value <= best_cut_value:\n",
    "            left_set[feature].append(value)\n",
    "            left_set[class_label].append(df[class_label][i])\n",
    "        else:\n",
    "            right_set[feature].append(value)\n",
    "            right_set[class_label].append(df[class_label][i])\n",
    "    # 切割完，若任一區間沒有資料，或分割後區間內容與分割前一樣，代表分割沒有幫助 \n",
    "    if (len(left_set[feature]) == 0 or len(right_set[feature]) == 0 or\n",
    "        len(left_set[feature]) == len(df[feature]) or len(right_set[feature]) == len(df[feature])):\n",
    "        return []\n",
    "    # 計算 MDLPC criterion 的 threshold\n",
    "\n",
    "    # 計算初始區間、切割後的左右區間個包含的 class 種類數量\n",
    "    k = len(set(df[class_label]))\n",
    "    k1 = len(set(left_set[class_label]))\n",
    "    k2 = len(set(right_set[class_label]))\n",
    "    N = len(df[feature])\n",
    "    H_S = Ent(df, class_label)\n",
    "    H_l = Ent(left_set, class_label)\n",
    "    H_r = Ent(right_set, class_label)\n",
    "    delta = log2(3 ** k - 2) - (k * H_S - k1 * H_l - k2 * H_r)\n",
    "    threshold = log2(N - 1) / N + delta / N\n",
    "\n",
    "    # 最好的切割點的 gain 未超過 threshold，停止\n",
    "    if best_info_gain <= threshold :\n",
    "        return []\n",
    "    \n",
    "    cut_points.append(best_cut_value)\n",
    "    # 對左右區間遞迴做 entropy_base 切割\n",
    "    split(left_set,feature,class_label,cut_points)\n",
    "    split(right_set,feature,class_label,cut_points)\n",
    "    return cut_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算區間內類別值的 entropy\n",
    "def Ent(df,class_label):\n",
    "    class_value = df[class_label]\n",
    "    cnt = Counter(class_value)\n",
    "    prob = [count / len(class_value) for i,count in cnt.items()]\n",
    "    Ent = -sum(p * log2(p) for p in prob) \n",
    "    return Ent\n",
    "\n",
    "# cut_index 為資料被分割成兩個 subset 時，「右側區間的起始 index」，此函式計算切割後的資訊增益\n",
    "def info_gain(df, cutpoint, feature, class_label):\n",
    "    total = len(df[class_label])\n",
    "\n",
    "    # 依據 feature 值是否 <= midpoint 來分割資料\n",
    "    left = {class_label: [df[class_label][i] for i in range(len(df[feature])) if float(df[feature][i]) <= cutpoint]}\n",
    "    right = {class_label: [df[class_label][i] for i in range(len(df[feature])) if float(df[feature][i]) > cutpoint]}\n",
    "\n",
    "    # 若任一邊為空，代表不是有效切點，資訊增益為 0\n",
    "    if not left[class_label] or not right[class_label]:\n",
    "        return 0\n",
    "\n",
    "    Ent_cut = (len(left[class_label]) / total) * Ent(left, class_label) + (len(right[class_label]) / total) * Ent(right, class_label)\n",
    "\n",
    "    return Ent(df, class_label) - Ent_cut\n",
    "\n",
    "# 找 feature 的最佳切點\n",
    "def find_cut_point(df, feature, class_label):\n",
    "    best_info_gain = -1\n",
    "    best_cut_value = None\n",
    "\n",
    "    # 整理為 (value, label) 配對並排序\n",
    "    value_label = sorted([(float(df[feature][i]), df[class_label][i], i) for i in range(len(df[feature]))], key=lambda x: (x[0],x[2]))\n",
    "    for i in range(1, len(value_label)):\n",
    "        if value_label[i][1] != value_label[i - 1][1] and value_label[i][0] != value_label[i - 1][0]:  # 類別不同才考慮切點\n",
    "            midpoint = (value_label[i][0] + value_label[i - 1][0]) / 2\n",
    "            cur_info_gain = info_gain(df, midpoint, feature, class_label)\n",
    "            if cur_info_gain > best_info_gain:\n",
    "                \n",
    "                best_info_gain = cur_info_gain\n",
    "                best_cut_value = midpoint\n",
    "\n",
    "    return best_info_gain, best_cut_value\n",
    "\n",
    "# 對整個 df 的 feature 欄位做 entropy_base 切割，找所有切點，返回切割點 list\n",
    "def split(df,feature,class_label,cut_points):\n",
    "    # 若傳進來的 cut_points 為空，代表區間無可用的切割點\n",
    "    if cut_points is None:\n",
    "        cut_points = []\n",
    "    best_info_gain,best_cut_value = find_cut_point(df,feature,class_label)\n",
    "\n",
    "    # 若區間的 class 值或 feature 值都是一樣的，或只剩一個 instance，代表切割無意義\n",
    "    if (len(df[class_label]) <= 1 or best_cut_value is None or\n",
    "        len(set(df[class_label])) == 1 or len(set(df[feature])) == 1):\n",
    "        return []\n",
    "    \n",
    "    # 創造兩個 dict of list 儲存切割後的 feature 與 class 欄位\n",
    "    left_set = {feature : [],class_label : []}\n",
    "    right_set = {feature : [],class_label : []}\n",
    "    for i in range(len(df[feature])):\n",
    "        # 分配 instances 至對應的區間\n",
    "        value = float(df[feature][i])\n",
    "        if value <= best_cut_value:\n",
    "            left_set[feature].append(value)\n",
    "            left_set[class_label].append(df[class_label][i])\n",
    "        else:\n",
    "            right_set[feature].append(value)\n",
    "            right_set[class_label].append(df[class_label][i])\n",
    "    # 切割完，若任一區間沒有資料，或分割後區間內容與分割前一樣，代表分割沒有幫助 \n",
    "    if (len(left_set[feature]) == 0 or len(right_set[feature]) == 0 or\n",
    "        len(left_set[feature]) == len(df[feature]) or len(right_set[feature]) == len(df[feature])):\n",
    "        return []\n",
    "    # 計算 MDLPC criterion 的 threshold\n",
    "\n",
    "    # 計算初始區間、切割後的左右區間個包含的 class 種類數量\n",
    "    k = len(set(df[class_label]))\n",
    "    k1 = len(set(left_set[class_label]))\n",
    "    k2 = len(set(right_set[class_label]))\n",
    "    N = len(df[feature])\n",
    "    H_S = Ent(df, class_label)\n",
    "    H_l = Ent(left_set, class_label)\n",
    "    H_r = Ent(right_set, class_label)\n",
    "    delta = log2(3 ** k - 2) - (k * H_S - k1 * H_l - k2 * H_r)\n",
    "    threshold = log2(N - 1) / N + delta / N\n",
    "\n",
    "    # 最好的切割點的 gain 未超過 threshold，停止\n",
    "    if best_info_gain <= threshold :\n",
    "        return []\n",
    "    \n",
    "    cut_points.append(best_cut_value)\n",
    "    # 對左右區間遞迴做 entropy_base 切割\n",
    "    split(left_set,feature,class_label,cut_points)\n",
    "    split(right_set,feature,class_label,cut_points)\n",
    "    return cut_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with entropy base discretization:\n",
      "[1.517335, 1.517985]\n",
      "=========================================================\n",
      "Na with entropy base discretization:\n",
      "[14.065]\n",
      "=========================================================\n",
      "Mg with entropy base discretization:\n",
      "[2.6950000000000003]\n",
      "=========================================================\n",
      "Al with entropy base discretization:\n",
      "[1.39, 1.775]\n",
      "=========================================================\n",
      "Si with entropy base discretization:\n",
      "[]\n",
      "=========================================================\n",
      "K with entropy base discretization:\n",
      "[0.055, 0.615, 0.745]\n",
      "=========================================================\n",
      "Ca with entropy base discretization:\n",
      "[7.02, 8.29, 10.075]\n",
      "=========================================================\n",
      "Ba with entropy base discretization:\n",
      "[0.335]\n",
      "=========================================================\n",
      "Fe with entropy base discretization:\n",
      "[]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# 照上述的切點做 discretization\n",
    "def discretize_entropy_base(df,features,class_label):\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        print(f'{feature} with entropy base discretization:')    \n",
    "        cut_points = []\n",
    "        cut_points = split(df,feature,\"class\",cut_points)\n",
    "        # 若沒有切點，代表整個特徵內容會被離散為同個類別\n",
    "        if cut_points == []:\n",
    "            ent_base_res = [1] * len(df[feature])\n",
    "            bin_results[feature] = ent_base_res\n",
    "        else:\n",
    "            cut_points = sorted(cut_points)\n",
    "            org_value = df[feature]     # 紀錄特徵值的原始值\n",
    "            ent_base_res = []           # 紀錄離散化後的特徵值\n",
    "            # 對每個 feature 值做離散化分配\n",
    "            for value in org_value:\n",
    "                value = float(value)\n",
    "                for i,cut_point in enumerate(cut_points):\n",
    "                    # 只有一個切割點，只會被切成兩個區間\n",
    "                    if len(cut_points) == 1:\n",
    "                        if value <= cut_point:\n",
    "                            ent_base_res.append(i + 1)\n",
    "                        else:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                    # 有兩個以上的切點\n",
    "                    else:\n",
    "                        if i == 0 and value <= cut_point:\n",
    "                            ent_base_res.append(i + 1)\n",
    "                        elif i == 0 and value > cut_point and value <= cut_points[i + 1]:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                        elif i == len(cut_points) - 1 and value > cut_point:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                        elif i != 0 and i != len(cut_points) - 1 and value > cut_point and value <= cut_points[i + 1]:\n",
    "                            ent_base_res.append(i + 2)               \n",
    "            bin_results[feature] = ent_base_res\n",
    "        print(f\"{cut_points}\")\n",
    "        print(\"=========================================================\")\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "entropy_base_df = discretize_entropy_base(df,X,'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection:\n",
      "Pass1: best_feature_subset = ['Mg'] , Goodness = 0.37040112253047947\n",
      "Forward Selection:\n",
      "Pass2: best_feature_subset = ['Mg', 'Al'] , Goodness = 0.44111979963245035\n",
      "Forward Selection:\n",
      "Pass3: best_feature_subset = ['Mg', 'Al', 'Ca'] , Goodness = 0.4684573941293161\n",
      "Forward Selection:\n",
      "Pass4: best_feature_subset = ['Mg', 'Al', 'Ca', 'Ba'] , Goodness = 0.4923013144646698\n",
      "Forward Selection:\n",
      "Pass5: best_feature_subset = ['Mg', 'Al', 'Ca', 'Ba', 'K'] , Goodness = 0.5084334732749055\n",
      "Forward Selection:\n",
      "Final select features: ['Mg', 'Al', 'Ca', 'Ba', 'K'], Goodness = 0.5084334732749055\n",
      "=================================================================================================\n",
      "Backward Selection:\n",
      "Pass1: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.5106082375130458\n",
      "remove feature: Si\n",
      "Backward Selection:\n",
      "Pass2: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'] , Goodness = 0.5106082375130458\n",
      "remove feature: Fe\n",
      "Backward Selection:\n",
      "Final select features: ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'], Goodness = 0.5106082375130458\n"
     ]
    }
   ],
   "source": [
    "# 對所有特徵離散化後的資料集做 forward selection、backward selection\n",
    "forward_selection(entropy_base_df, X, 'class')\n",
    "print(\"=================================================================================================\")\n",
    "backward_selection(entropy_base_df, X, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### equal_width_df、equal_frequency_df、entropy_base_df 為所有特徵皆以離散化後的 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算 P(feature_name = X_i | class_name = C_j) 的 Laplace Estimation 值\n",
    "def laplace_est(df, feature_name, class_name, X_i, C_j):\n",
    "    # 取出特定特徵與類別的所有值\n",
    "    feature_values = df[feature_name]\n",
    "    class_values = df[class_name]\n",
    "    # 取出當 class = C_j 類別時的 feature_name 可能值 f\n",
    "    filter_feature_Cj = [f for f, C in zip(feature_values, class_values) if C == C_j]\n",
    "    # N_ij 為 feature_name 值 = X_i | class_name = C_j 的樣本個數\n",
    "    N_ij = filter_feature_Cj.count(X_i)\n",
    "    # N_j 是類別為 Cj 的樣本總數\n",
    "    N_j = len(filter_feature_Cj)\n",
    "    # k 為特徵 feature_name 的可能值個數\n",
    "    k = len(set(feature_values))\n",
    "    return (N_ij + 1) / (N_j + k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以 naive bayes 預測資料\n",
    "def naive_bayes(df,feature_set,class_name):\n",
    "    prediction = [] # 儲存資料集預測結果\n",
    "    # 計算每個類別的先驗機率 P(class_name = C_j)\n",
    "    N = len(df[class_name])\n",
    "    # 取出類別的可能值\n",
    "    class_values = list(set(df[class_name]))\n",
    "    # 計算每個唯一類別值的先驗機率\n",
    "    prior_C = {c : df[class_name].count(c) / N for c in class_values}\n",
    "    # 透過 feature_set 的值預測每筆樣本\n",
    "    # 每筆資料都存在一組 X (每個特徵的值)，針對每個資料的所有特徵值計算 P( Xi | Cj)\n",
    "    for i in range(N):\n",
    "        instance = {f : df[f][i] for f in feature_set} \n",
    "        max_prob = -1  # 儲存類別的預測可能機率\n",
    "        predict_c = None\n",
    "        # 針對每個類別 Cj 計算 SumProduct(P(Xi | Cj)*P(Cj))\n",
    "        for Cj in class_values:\n",
    "            p_Xi_Cj = 1\n",
    "            # 計算每個 feature 對應的 P(Xi | Cj)\n",
    "            for feature in feature_set:\n",
    "                p_Xi_Cj *= laplace_est(df,feature,class_name,instance[feature],Cj)\n",
    "            # 最後計算後驗機率\n",
    "            posterior = prior_C[Cj] * p_Xi_Cj\n",
    "            # 若此類別的後驗機率 > 先前的最大機率\n",
    "            if posterior > max_prob:\n",
    "                max_prob = posterior\n",
    "                predict_c = Cj  # 改為預測 Cj 類別\n",
    "        prediction.append(predict_c)   \n",
    "    return prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = (entropy_base_df,X,'class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算預測準確率\n",
    "def cal_accuracy(df,class_name,prediction):\n",
    "    correct = 0\n",
    "    for i in range(len(df[class_name])):\n",
    "        if df[class_name][i] == prediction[i]:\n",
    "            correct += 1\n",
    "    return correct / len(df[class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = naive_bayes(df,X,'class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7616822429906542"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = naive_bayes(entropy_base_df,X,'class')\n",
    "cal_accuracy(df,'class',pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"RI\",\"Na\",\"Mg\",\"Al\",\"Si\",\"K\",\"Ca\",\"Ba\",\"Fe\",\"class\"]\n",
    "X = columns.copy()\n",
    "X.remove(\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 Naive Bayse 方法執行 Feature Selection\n",
    "def feature_selection_NB(df,features,class_name):\n",
    "    print(\"Forward Selelcton:\")\n",
    "    select_features = []\n",
    "    remaining_features = features.copy()\n",
    "    best_accuracy = 0.0\n",
    "    class_values = df[class_name]\n",
    "    N = len(class_values)\n",
    "    i = 1\n",
    "    # 當還有特徵未被選取\n",
    "    while len(remaining_features) > 0:\n",
    "        Accuracy = [] # 儲存該輪每個特徵加進來後的個別準確率\n",
    "        # 測試每個還未被選取的特徵\n",
    "        \n",
    "        for feature in remaining_features:\n",
    "            temp_features = select_features + [feature]\n",
    "            # 使用當前特徵 temp_features 去做 naive bayse 預測\n",
    "            prediction = naive_bayes(df,temp_features,class_name)\n",
    "            acc = cal_accuracy(df,class_name,prediction)\n",
    "            print(f\"Features : {temp_features} , Acc : {acc}\")\n",
    "            Accuracy.append((feature,acc))\n",
    "\n",
    "        # 依照準確率由大到小排序\n",
    "        Accuracy.sort(key = lambda x : x[1],reverse = True)\n",
    "        best_feature, best_new_acc = Accuracy[0][0],Accuracy[0][1]  # 得出該輪準確率最好的特徵與 Accuracy\n",
    "        # 若準確率大於上輪最優準確率\n",
    "        if best_new_acc > best_accuracy:\n",
    "            best_accuracy = best_new_acc\n",
    "            select_features = select_features + [best_feature]      # 選該特徵進來\n",
    "            remaining_features.remove(best_feature)\n",
    "            print(\"-------------------------------------------------------------------\")\n",
    "            print(f\"Pass{i}: best_feature_subset = {select_features} , Accuracy = {best_accuracy}\")\n",
    "\n",
    "            i += 1\n",
    "        # 當準確率無法再提升即停止\n",
    "        else:\n",
    "            print(\"Acccuracy stop improving\")\n",
    "            break\n",
    "    print(f\"Final select features: {select_features}, Accuracy = {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selelcton:\n",
      "Features : ['RI'] , Acc : 0.411214953271028\n",
      "Features : ['Na'] , Acc : 0.46261682242990654\n",
      "Features : ['Mg'] , Acc : 0.4766355140186916\n",
      "Features : ['Al'] , Acc : 0.5186915887850467\n",
      "Features : ['Si'] , Acc : 0.42990654205607476\n",
      "Features : ['K'] , Acc : 0.4485981308411215\n",
      "Features : ['Ca'] , Acc : 0.48598130841121495\n",
      "Features : ['Ba'] , Acc : 0.4766355140186916\n",
      "Features : ['Fe'] , Acc : 0.37850467289719625\n",
      "-------------------------------------------------------------------\n",
      "Pass1: best_feature_subset = ['Al'] , Accuracy = 0.5186915887850467\n",
      "Features : ['Al', 'RI'] , Acc : 0.5327102803738317\n",
      "Features : ['Al', 'Na'] , Acc : 0.5467289719626168\n",
      "Features : ['Al', 'Mg'] , Acc : 0.5420560747663551\n",
      "Features : ['Al', 'Si'] , Acc : 0.5420560747663551\n",
      "Features : ['Al', 'K'] , Acc : 0.5560747663551402\n",
      "Features : ['Al', 'Ca'] , Acc : 0.602803738317757\n",
      "Features : ['Al', 'Ba'] , Acc : 0.5560747663551402\n",
      "Features : ['Al', 'Fe'] , Acc : 0.5420560747663551\n",
      "-------------------------------------------------------------------\n",
      "Pass2: best_feature_subset = ['Al', 'Ca'] , Accuracy = 0.602803738317757\n",
      "Features : ['Al', 'Ca', 'RI'] , Acc : 0.6074766355140186\n",
      "Features : ['Al', 'Ca', 'Na'] , Acc : 0.6261682242990654\n",
      "Features : ['Al', 'Ca', 'Mg'] , Acc : 0.6635514018691588\n",
      "Features : ['Al', 'Ca', 'Si'] , Acc : 0.602803738317757\n",
      "Features : ['Al', 'Ca', 'K'] , Acc : 0.5887850467289719\n",
      "Features : ['Al', 'Ca', 'Ba'] , Acc : 0.6355140186915887\n",
      "Features : ['Al', 'Ca', 'Fe'] , Acc : 0.6308411214953271\n",
      "-------------------------------------------------------------------\n",
      "Pass3: best_feature_subset = ['Al', 'Ca', 'Mg'] , Accuracy = 0.6635514018691588\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI'] , Acc : 0.6728971962616822\n",
      "Features : ['Al', 'Ca', 'Mg', 'Na'] , Acc : 0.6588785046728972\n",
      "Features : ['Al', 'Ca', 'Mg', 'Si'] , Acc : 0.6495327102803738\n",
      "Features : ['Al', 'Ca', 'Mg', 'K'] , Acc : 0.6448598130841121\n",
      "Features : ['Al', 'Ca', 'Mg', 'Ba'] , Acc : 0.6728971962616822\n",
      "Features : ['Al', 'Ca', 'Mg', 'Fe'] , Acc : 0.6588785046728972\n",
      "-------------------------------------------------------------------\n",
      "Pass4: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI'] , Accuracy = 0.6728971962616822\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na'] , Acc : 0.6962616822429907\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Si'] , Acc : 0.6588785046728972\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'K'] , Acc : 0.6822429906542056\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Ba'] , Acc : 0.6869158878504673\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Fe'] , Acc : 0.6682242990654206\n",
      "-------------------------------------------------------------------\n",
      "Pass5: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Na'] , Accuracy = 0.6962616822429907\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na', 'Si'] , Acc : 0.6869158878504673\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K'] , Acc : 0.7009345794392523\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na', 'Ba'] , Acc : 0.6915887850467289\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na', 'Fe'] , Acc : 0.6915887850467289\n",
      "-------------------------------------------------------------------\n",
      "Pass6: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K'] , Accuracy = 0.7009345794392523\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K', 'Si'] , Acc : 0.6915887850467289\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K', 'Ba'] , Acc : 0.7102803738317757\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K', 'Fe'] , Acc : 0.719626168224299\n",
      "-------------------------------------------------------------------\n",
      "Pass7: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K', 'Fe'] , Accuracy = 0.719626168224299\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K', 'Fe', 'Si'] , Acc : 0.7102803738317757\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K', 'Fe', 'Ba'] , Acc : 0.719626168224299\n",
      "Acccuracy stop improving\n",
      "Final select features: ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K', 'Fe'], Accuracy = 0.719626168224299\n"
     ]
    }
   ],
   "source": [
    "feature_selection_NB(equal_width_df,X,'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(69.81, 2.0, 106),\n",
       " (69.89, 5.0, 163),\n",
       " (70.16, 2.0, 107),\n",
       " (70.26, 7.0, 188),\n",
       " (70.43, 7.0, 189),\n",
       " (70.48, 5.0, 171),\n",
       " (70.57, 2.0, 103),\n",
       " (70.7, 5.0, 172),\n",
       " (71.15, 2.0, 104),\n",
       " (71.24, 2.0, 131),\n",
       " (71.25, 7.0, 186),\n",
       " (71.35, 1.0, 63),\n",
       " (71.36, 1.0, 17),\n",
       " (71.36, 3.0, 162),\n",
       " (71.5, 3.0, 151),\n",
       " (71.57, 1.0, 47),\n",
       " (71.72, 1.0, 61),\n",
       " (71.75, 1.0, 50),\n",
       " (71.76, 1.0, 43),\n",
       " (71.76, 2.0, 129),\n",
       " (71.77, 1.0, 38),\n",
       " (71.77, 1.0, 39),\n",
       " (71.78, 1.0, 0),\n",
       " (71.79, 1.0, 62),\n",
       " (71.79, 3.0, 157),\n",
       " (71.81, 1.0, 36),\n",
       " (71.81, 2.0, 133),\n",
       " (71.87, 2.0, 70),\n",
       " (71.94, 3.0, 159),\n",
       " (71.95, 1.0, 45),\n",
       " (71.96, 1.0, 65),\n",
       " (71.96, 2.0, 71),\n",
       " (71.99, 1.0, 48),\n",
       " (71.99, 1.0, 69),\n",
       " (71.99, 2.0, 127),\n",
       " (72.01, 1.0, 64),\n",
       " (72.02, 1.0, 21),\n",
       " (72.02, 2.0, 112),\n",
       " (72.04, 3.0, 158),\n",
       " (72.06, 2.0, 117),\n",
       " (72.08, 1.0, 8),\n",
       " (72.08, 1.0, 49),\n",
       " (72.12, 1.0, 18),\n",
       " (72.14, 3.0, 160),\n",
       " (72.18, 2.0, 128),\n",
       " (72.18, 5.0, 174),\n",
       " (72.19, 2.0, 105),\n",
       " (72.19, 2.0, 130),\n",
       " (72.2, 1.0, 68),\n",
       " (72.22, 1.0, 66),\n",
       " (72.22, 5.0, 170),\n",
       " (72.25, 5.0, 173),\n",
       " (72.26, 2.0, 95),\n",
       " (72.28, 2.0, 84),\n",
       " (72.28, 2.0, 90),\n",
       " (72.32, 1.0, 67),\n",
       " (72.33, 2.0, 116),\n",
       " (72.33, 2.0, 135),\n",
       " (72.34, 2.0, 96),\n",
       " (72.36, 1.0, 51),\n",
       " (72.37, 6.0, 176),\n",
       " (72.38, 2.0, 115),\n",
       " (72.38, 6.0, 178),\n",
       " (72.38, 7.0, 187),\n",
       " (72.39, 2.0, 76),\n",
       " (72.4, 2.0, 120),\n",
       " (72.44, 2.0, 114),\n",
       " (72.45, 2.0, 119),\n",
       " (72.48, 3.0, 150),\n",
       " (72.49, 2.0, 132),\n",
       " (72.5, 6.0, 181),\n",
       " (72.51, 2.0, 125),\n",
       " (72.52, 2.0, 123),\n",
       " (72.53, 2.0, 118),\n",
       " (72.54, 2.0, 140),\n",
       " (72.55, 2.0, 113),\n",
       " (72.57, 2.0, 145),\n",
       " (72.61, 1.0, 3),\n",
       " (72.61, 3.0, 154),\n",
       " (72.61, 7.0, 209),\n",
       " (72.64, 1.0, 26),\n",
       " (72.64, 1.0, 60),\n",
       " (72.64, 3.0, 156),\n",
       " (72.65, 2.0, 87),\n",
       " (72.65, 3.0, 161),\n",
       " (72.66, 2.0, 80),\n",
       " (72.67, 2.0, 108),\n",
       " (72.67, 3.0, 147),\n",
       " (72.67, 6.0, 179),\n",
       " (72.69, 3.0, 153),\n",
       " (72.69, 5.0, 164),\n",
       " (72.7, 3.0, 148),\n",
       " (72.72, 1.0, 46),\n",
       " (72.72, 2.0, 85),\n",
       " (72.72, 2.0, 143),\n",
       " (72.73, 1.0, 1),\n",
       " (72.73, 1.0, 19),\n",
       " (72.74, 1.0, 35),\n",
       " (72.74, 6.0, 182),\n",
       " (72.75, 1.0, 20),\n",
       " (72.75, 2.0, 124),\n",
       " (72.75, 2.0, 126),\n",
       " (72.76, 1.0, 42),\n",
       " (72.76, 6.0, 177),\n",
       " (72.77, 3.0, 146),\n",
       " (72.78, 2.0, 134),\n",
       " (72.79, 1.0, 22),\n",
       " (72.79, 1.0, 59),\n",
       " (72.81, 2.0, 82),\n",
       " (72.81, 7.0, 185),\n",
       " (72.83, 2.0, 77),\n",
       " (72.83, 2.0, 141),\n",
       " (72.84, 1.0, 52),\n",
       " (72.84, 2.0, 122),\n",
       " (72.85, 1.0, 24),\n",
       " (72.85, 1.0, 53),\n",
       " (72.85, 7.0, 208),\n",
       " (72.86, 1.0, 29),\n",
       " (72.86, 2.0, 79),\n",
       " (72.86, 5.0, 165),\n",
       " (72.86, 7.0, 207),\n",
       " (72.87, 2.0, 73),\n",
       " (72.87, 2.0, 83),\n",
       " (72.87, 2.0, 101),\n",
       " (72.88, 2.0, 78),\n",
       " (72.89, 2.0, 88),\n",
       " (72.89, 3.0, 149),\n",
       " (72.92, 2.0, 99),\n",
       " (72.95, 1.0, 34),\n",
       " (72.95, 1.0, 44),\n",
       " (72.95, 1.0, 57),\n",
       " (72.96, 1.0, 37),\n",
       " (72.96, 2.0, 121),\n",
       " (72.96, 2.0, 137),\n",
       " (72.97, 1.0, 5),\n",
       " (72.97, 1.0, 32),\n",
       " (72.97, 2.0, 144),\n",
       " (72.98, 1.0, 54),\n",
       " (72.98, 1.0, 56),\n",
       " (72.99, 1.0, 2),\n",
       " (72.99, 1.0, 9),\n",
       " (72.99, 1.0, 58),\n",
       " (72.99, 7.0, 205),\n",
       " (73.0, 1.0, 25),\n",
       " (73.0, 3.0, 152),\n",
       " (73.01, 1.0, 11),\n",
       " (73.01, 2.0, 142),\n",
       " (73.01, 3.0, 155),\n",
       " (73.02, 1.0, 23),\n",
       " (73.02, 7.0, 193),\n",
       " (73.03, 1.0, 40),\n",
       " (73.03, 5.0, 169),\n",
       " (73.04, 1.0, 27),\n",
       " (73.05, 7.0, 199),\n",
       " (73.06, 2.0, 91),\n",
       " (73.06, 7.0, 210),\n",
       " (73.07, 2.0, 136),\n",
       " (73.08, 1.0, 4),\n",
       " (73.08, 1.0, 30),\n",
       " (73.08, 2.0, 111),\n",
       " (73.09, 1.0, 6),\n",
       " (73.1, 2.0, 72),\n",
       " (73.1, 2.0, 93),\n",
       " (73.1, 7.0, 190),\n",
       " (73.1, 7.0, 198),\n",
       " (73.11, 1.0, 16),\n",
       " (73.11, 2.0, 74),\n",
       " (73.11, 7.0, 203),\n",
       " (73.11, 7.0, 206),\n",
       " (73.12, 2.0, 75),\n",
       " (73.14, 2.0, 139),\n",
       " (73.15, 1.0, 28),\n",
       " (73.17, 2.0, 81),\n",
       " (73.2, 1.0, 10),\n",
       " (73.2, 1.0, 41),\n",
       " (73.21, 1.0, 13),\n",
       " (73.21, 2.0, 98),\n",
       " (73.21, 2.0, 110),\n",
       " (73.23, 2.0, 89),\n",
       " (73.23, 7.0, 196),\n",
       " (73.24, 1.0, 7),\n",
       " (73.24, 1.0, 15),\n",
       " (73.25, 2.0, 86),\n",
       " (73.26, 2.0, 92),\n",
       " (73.27, 1.0, 31),\n",
       " (73.27, 2.0, 100),\n",
       " (73.28, 1.0, 12),\n",
       " (73.28, 2.0, 94),\n",
       " (73.28, 7.0, 191),\n",
       " (73.28, 7.0, 197),\n",
       " (73.29, 1.0, 14),\n",
       " (73.29, 7.0, 194),\n",
       " (73.3, 7.0, 204),\n",
       " (73.36, 2.0, 138),\n",
       " (73.36, 7.0, 213),\n",
       " (73.39, 1.0, 33),\n",
       " (73.39, 5.0, 175),\n",
       " (73.39, 7.0, 195),\n",
       " (73.42, 7.0, 211),\n",
       " (73.44, 5.0, 166),\n",
       " (73.46, 7.0, 192),\n",
       " (73.48, 6.0, 183),\n",
       " (73.5, 7.0, 200),\n",
       " (73.55, 2.0, 97),\n",
       " (73.61, 7.0, 212),\n",
       " (73.7, 1.0, 55),\n",
       " (73.72, 7.0, 202),\n",
       " (73.75, 5.0, 167),\n",
       " (73.81, 2.0, 102),\n",
       " (73.88, 5.0, 168),\n",
       " (74.45, 2.0, 109),\n",
       " (74.55, 6.0, 180),\n",
       " (75.18, 7.0, 201),\n",
       " (75.41, 6.0, 184)]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_label = sorted([(float(df['Si'][i]), df['class'][i], i) for i in range(len(df['Si']))], key=lambda x: (x[0],x[2]))\n",
    "value_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 2.0, 105),\n",
       " (0.0, 2.0, 106),\n",
       " (0.0, 2.0, 107),\n",
       " (0.0, 2.0, 108),\n",
       " (0.0, 2.0, 109),\n",
       " (0.0, 2.0, 110),\n",
       " (0.0, 2.0, 111),\n",
       " (0.0, 2.0, 112),\n",
       " (0.0, 2.0, 131),\n",
       " (0.0, 5.0, 167),\n",
       " (0.0, 5.0, 168),\n",
       " (0.0, 5.0, 169),\n",
       " (0.0, 5.0, 170),\n",
       " (0.0, 5.0, 171),\n",
       " (0.0, 5.0, 172),\n",
       " (0.0, 5.0, 173),\n",
       " (0.0, 6.0, 182),\n",
       " (0.0, 6.0, 183),\n",
       " (0.0, 6.0, 184),\n",
       " (0.0, 7.0, 191),\n",
       " (0.0, 7.0, 192),\n",
       " (0.0, 7.0, 193),\n",
       " (0.0, 7.0, 194),\n",
       " (0.0, 7.0, 195),\n",
       " (0.0, 7.0, 196),\n",
       " (0.0, 7.0, 197),\n",
       " (0.0, 7.0, 198),\n",
       " (0.0, 7.0, 199),\n",
       " (0.0, 7.0, 200),\n",
       " (0.0, 7.0, 201),\n",
       " (0.0, 7.0, 202),\n",
       " (0.0, 7.0, 203),\n",
       " (0.0, 7.0, 204),\n",
       " (0.0, 7.0, 205),\n",
       " (0.0, 7.0, 206),\n",
       " (0.0, 7.0, 207),\n",
       " (0.0, 7.0, 208),\n",
       " (0.0, 7.0, 209),\n",
       " (0.0, 7.0, 210),\n",
       " (0.0, 7.0, 211),\n",
       " (0.0, 7.0, 212),\n",
       " (0.0, 7.0, 213),\n",
       " (0.33, 5.0, 175),\n",
       " (0.78, 6.0, 181),\n",
       " (1.01, 2.0, 130),\n",
       " (1.35, 2.0, 129),\n",
       " (1.61, 5.0, 174),\n",
       " (1.71, 5.0, 166),\n",
       " (1.74, 6.0, 180),\n",
       " (1.78, 7.0, 190),\n",
       " (1.83, 7.0, 189),\n",
       " (1.85, 5.0, 164),\n",
       " (1.88, 5.0, 165),\n",
       " (2.09, 2.0, 128),\n",
       " (2.19, 6.0, 179),\n",
       " (2.2, 7.0, 188),\n",
       " (2.24, 6.0, 178),\n",
       " (2.28, 2.0, 127),\n",
       " (2.39, 6.0, 176),\n",
       " (2.41, 6.0, 177),\n",
       " (2.68, 5.0, 163),\n",
       " (2.71, 1.0, 55),\n",
       " (2.72, 2.0, 101),\n",
       " (2.76, 2.0, 102),\n",
       " (2.81, 1.0, 54),\n",
       " (2.84, 1.0, 53),\n",
       " (2.85, 2.0, 100),\n",
       " (2.87, 1.0, 52),\n",
       " (2.88, 2.0, 98),\n",
       " (2.9, 2.0, 104),\n",
       " (2.96, 2.0, 99),\n",
       " (3.09, 2.0, 84),\n",
       " (3.15, 2.0, 103),\n",
       " (3.18, 2.0, 144),\n",
       " (3.2, 7.0, 185),\n",
       " (3.25, 2.0, 97),\n",
       " (3.26, 7.0, 186),\n",
       " (3.33, 1.0, 51),\n",
       " (3.33, 2.0, 94),\n",
       " (3.34, 2.0, 93),\n",
       " (3.34, 3.0, 160),\n",
       " (3.34, 7.0, 187),\n",
       " (3.35, 1.0, 49),\n",
       " (3.36, 3.0, 159),\n",
       " (3.37, 1.0, 46),\n",
       " (3.39, 1.0, 42),\n",
       " (3.39, 3.0, 156),\n",
       " (3.4, 3.0, 153),\n",
       " (3.4, 3.0, 155),\n",
       " (3.41, 2.0, 92),\n",
       " (3.41, 3.0, 158),\n",
       " (3.42, 1.0, 41),\n",
       " (3.43, 1.0, 12),\n",
       " (3.43, 1.0, 44),\n",
       " (3.43, 2.0, 95),\n",
       " (3.44, 2.0, 91),\n",
       " (3.45, 1.0, 35),\n",
       " (3.45, 2.0, 81),\n",
       " (3.45, 3.0, 150),\n",
       " (3.46, 1.0, 10),\n",
       " (3.47, 1.0, 33),\n",
       " (3.47, 1.0, 56),\n",
       " (3.47, 2.0, 143),\n",
       " (3.48, 1.0, 26),\n",
       " (3.48, 1.0, 27),\n",
       " (3.48, 1.0, 32),\n",
       " (3.48, 1.0, 37),\n",
       " (3.48, 1.0, 45),\n",
       " (3.48, 1.0, 57),\n",
       " (3.48, 2.0, 89),\n",
       " (3.48, 2.0, 123),\n",
       " (3.49, 1.0, 29),\n",
       " (3.49, 2.0, 86),\n",
       " (3.49, 2.0, 87),\n",
       " (3.5, 1.0, 24),\n",
       " (3.5, 1.0, 31),\n",
       " (3.5, 1.0, 40),\n",
       " (3.5, 2.0, 88),\n",
       " (3.51, 2.0, 142),\n",
       " (3.52, 1.0, 28),\n",
       " (3.52, 2.0, 78),\n",
       " (3.52, 2.0, 79),\n",
       " (3.52, 2.0, 80),\n",
       " (3.52, 2.0, 83),\n",
       " (3.52, 2.0, 138),\n",
       " (3.52, 3.0, 149),\n",
       " (3.53, 1.0, 36),\n",
       " (3.53, 3.0, 147),\n",
       " (3.54, 1.0, 15),\n",
       " (3.54, 1.0, 19),\n",
       " (3.54, 1.0, 25),\n",
       " (3.54, 1.0, 34),\n",
       " (3.54, 2.0, 121),\n",
       " (3.54, 2.0, 122),\n",
       " (3.54, 2.0, 140),\n",
       " (3.54, 3.0, 161),\n",
       " (3.55, 1.0, 2),\n",
       " (3.55, 1.0, 20),\n",
       " (3.55, 2.0, 82),\n",
       " (3.56, 1.0, 13),\n",
       " (3.56, 1.0, 30),\n",
       " (3.56, 2.0, 74),\n",
       " (3.56, 2.0, 139),\n",
       " (3.57, 1.0, 23),\n",
       " (3.57, 2.0, 73),\n",
       " (3.57, 2.0, 119),\n",
       " (3.57, 3.0, 148),\n",
       " (3.58, 1.0, 8),\n",
       " (3.58, 1.0, 61),\n",
       " (3.58, 1.0, 68),\n",
       " (3.58, 1.0, 69),\n",
       " (3.58, 2.0, 75),\n",
       " (3.58, 2.0, 77),\n",
       " (3.58, 2.0, 85),\n",
       " (3.58, 3.0, 154),\n",
       " (3.59, 1.0, 14),\n",
       " (3.59, 1.0, 65),\n",
       " (3.59, 2.0, 72),\n",
       " (3.6, 1.0, 1),\n",
       " (3.6, 1.0, 6),\n",
       " (3.6, 1.0, 9),\n",
       " (3.61, 1.0, 5),\n",
       " (3.61, 1.0, 7),\n",
       " (3.61, 2.0, 76),\n",
       " (3.61, 2.0, 126),\n",
       " (3.62, 1.0, 4),\n",
       " (3.62, 1.0, 22),\n",
       " (3.62, 1.0, 60),\n",
       " (3.62, 2.0, 96),\n",
       " (3.62, 2.0, 137),\n",
       " (3.63, 2.0, 141),\n",
       " (3.64, 2.0, 118),\n",
       " (3.65, 1.0, 66),\n",
       " (3.65, 1.0, 67),\n",
       " (3.65, 3.0, 152),\n",
       " (3.66, 1.0, 11),\n",
       " (3.66, 1.0, 59),\n",
       " (3.66, 2.0, 125),\n",
       " (3.66, 3.0, 146),\n",
       " (3.67, 1.0, 16),\n",
       " (3.67, 2.0, 70),\n",
       " (3.67, 2.0, 145),\n",
       " (3.68, 2.0, 117),\n",
       " (3.68, 2.0, 124),\n",
       " (3.69, 1.0, 3),\n",
       " (3.7, 1.0, 47),\n",
       " (3.72, 1.0, 50),\n",
       " (3.73, 1.0, 18),\n",
       " (3.74, 1.0, 58),\n",
       " (3.74, 1.0, 64),\n",
       " (3.74, 2.0, 90),\n",
       " (3.75, 1.0, 21),\n",
       " (3.76, 2.0, 120),\n",
       " (3.76, 3.0, 157),\n",
       " (3.77, 1.0, 48),\n",
       " (3.78, 3.0, 162),\n",
       " (3.8, 2.0, 136),\n",
       " (3.81, 1.0, 63),\n",
       " (3.82, 1.0, 38),\n",
       " (3.82, 1.0, 39),\n",
       " (3.83, 2.0, 113),\n",
       " (3.84, 1.0, 43),\n",
       " (3.85, 1.0, 17),\n",
       " (3.85, 2.0, 134),\n",
       " (3.86, 1.0, 62),\n",
       " (3.87, 2.0, 71),\n",
       " (3.89, 2.0, 115),\n",
       " (3.9, 2.0, 116),\n",
       " (3.9, 2.0, 135),\n",
       " (3.9, 3.0, 151),\n",
       " (3.93, 2.0, 133),\n",
       " (3.97, 2.0, 114),\n",
       " (3.98, 2.0, 132),\n",
       " (4.49, 1.0, 0)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_label = sorted([(float(df['Mg'][i]), df['class'][i], i) for i in range(len(df['Mg']))], key=lambda x: (x[0],x[2]))\n",
    "value_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selelcton:\n",
      "Features : ['RI'] , Acc : 0.5373831775700935\n",
      "Features : ['Na'] , Acc : 0.49065420560747663\n",
      "Features : ['Mg'] , Acc : 0.514018691588785\n",
      "Features : ['Al'] , Acc : 0.5841121495327103\n",
      "Features : ['Si'] , Acc : 0.46261682242990654\n",
      "Features : ['K'] , Acc : 0.5327102803738317\n",
      "Features : ['Ca'] , Acc : 0.514018691588785\n",
      "Features : ['Ba'] , Acc : 0.4485981308411215\n",
      "Features : ['Fe'] , Acc : 0.37850467289719625\n",
      "-------------------------------------------------------------------\n",
      "Pass1: best_feature_subset = ['Al'] , Accuracy = 0.5841121495327103\n",
      "Features : ['Al', 'RI'] , Acc : 0.6588785046728972\n",
      "Features : ['Al', 'Na'] , Acc : 0.6308411214953271\n",
      "Features : ['Al', 'Mg'] , Acc : 0.6495327102803738\n",
      "Features : ['Al', 'Si'] , Acc : 0.6121495327102804\n",
      "Features : ['Al', 'K'] , Acc : 0.6588785046728972\n",
      "Features : ['Al', 'Ca'] , Acc : 0.6542056074766355\n",
      "Features : ['Al', 'Ba'] , Acc : 0.6214953271028038\n",
      "Features : ['Al', 'Fe'] , Acc : 0.6074766355140186\n",
      "-------------------------------------------------------------------\n",
      "Pass2: best_feature_subset = ['Al', 'RI'] , Accuracy = 0.6588785046728972\n",
      "Features : ['Al', 'RI', 'Na'] , Acc : 0.7383177570093458\n",
      "Features : ['Al', 'RI', 'Mg'] , Acc : 0.7336448598130841\n",
      "Features : ['Al', 'RI', 'Si'] , Acc : 0.6962616822429907\n",
      "Features : ['Al', 'RI', 'K'] , Acc : 0.7336448598130841\n",
      "Features : ['Al', 'RI', 'Ca'] , Acc : 0.7710280373831776\n",
      "Features : ['Al', 'RI', 'Ba'] , Acc : 0.705607476635514\n",
      "Features : ['Al', 'RI', 'Fe'] , Acc : 0.677570093457944\n",
      "-------------------------------------------------------------------\n",
      "Pass3: best_feature_subset = ['Al', 'RI', 'Ca'] , Accuracy = 0.7710280373831776\n",
      "Features : ['Al', 'RI', 'Ca', 'Na'] , Acc : 0.8130841121495327\n",
      "Features : ['Al', 'RI', 'Ca', 'Mg'] , Acc : 0.780373831775701\n",
      "Features : ['Al', 'RI', 'Ca', 'Si'] , Acc : 0.8177570093457944\n",
      "Features : ['Al', 'RI', 'Ca', 'K'] , Acc : 0.780373831775701\n",
      "Features : ['Al', 'RI', 'Ca', 'Ba'] , Acc : 0.7850467289719626\n",
      "Features : ['Al', 'RI', 'Ca', 'Fe'] , Acc : 0.7663551401869159\n",
      "-------------------------------------------------------------------\n",
      "Pass4: best_feature_subset = ['Al', 'RI', 'Ca', 'Si'] , Accuracy = 0.8177570093457944\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'Na'] , Acc : 0.822429906542056\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'Mg'] , Acc : 0.7990654205607477\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'K'] , Acc : 0.8271028037383178\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'Ba'] , Acc : 0.822429906542056\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'Fe'] , Acc : 0.8177570093457944\n",
      "-------------------------------------------------------------------\n",
      "Pass5: best_feature_subset = ['Al', 'RI', 'Ca', 'Si', 'K'] , Accuracy = 0.8271028037383178\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'K', 'Na'] , Acc : 0.8084112149532711\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'K', 'Mg'] , Acc : 0.8037383177570093\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'K', 'Ba'] , Acc : 0.8411214953271028\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'K', 'Fe'] , Acc : 0.8364485981308412\n",
      "-------------------------------------------------------------------\n",
      "Pass6: best_feature_subset = ['Al', 'RI', 'Ca', 'Si', 'K', 'Ba'] , Accuracy = 0.8411214953271028\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'K', 'Ba', 'Na'] , Acc : 0.8177570093457944\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'K', 'Ba', 'Mg'] , Acc : 0.8411214953271028\n",
      "Features : ['Al', 'RI', 'Ca', 'Si', 'K', 'Ba', 'Fe'] , Acc : 0.8411214953271028\n",
      "Acccuracy stop improving\n",
      "Final select features: ['Al', 'RI', 'Ca', 'Si', 'K', 'Ba'], Accuracy = 0.8411214953271028\n"
     ]
    }
   ],
   "source": [
    "feature_selection_NB(equal_frequency_df,X,'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selelcton:\n",
      "Features : ['RI'] , Acc : 0.5\n",
      "Features : ['Na'] , Acc : 0.4532710280373832\n",
      "Features : ['Mg'] , Acc : 0.4485981308411215\n",
      "Features : ['Al'] , Acc : 0.5887850467289719\n",
      "Features : ['Si'] , Acc : 0.35514018691588783\n",
      "Features : ['K'] , Acc : 0.5093457943925234\n",
      "Features : ['Ca'] , Acc : 0.5280373831775701\n",
      "Features : ['Ba'] , Acc : 0.4719626168224299\n",
      "Features : ['Fe'] , Acc : 0.35514018691588783\n",
      "-------------------------------------------------------------------\n",
      "Pass1: best_feature_subset = ['Al'] , Accuracy = 0.5887850467289719\n",
      "Features : ['Al', 'RI'] , Acc : 0.6074766355140186\n",
      "Features : ['Al', 'Na'] , Acc : 0.6214953271028038\n",
      "Features : ['Al', 'Mg'] , Acc : 0.6448598130841121\n",
      "Features : ['Al', 'Si'] , Acc : 0.5887850467289719\n",
      "Features : ['Al', 'K'] , Acc : 0.6214953271028038\n",
      "Features : ['Al', 'Ca'] , Acc : 0.6588785046728972\n",
      "Features : ['Al', 'Ba'] , Acc : 0.616822429906542\n",
      "Features : ['Al', 'Fe'] , Acc : 0.5887850467289719\n",
      "-------------------------------------------------------------------\n",
      "Pass2: best_feature_subset = ['Al', 'Ca'] , Accuracy = 0.6588785046728972\n",
      "Features : ['Al', 'Ca', 'RI'] , Acc : 0.7009345794392523\n",
      "Features : ['Al', 'Ca', 'Na'] , Acc : 0.6822429906542056\n",
      "Features : ['Al', 'Ca', 'Mg'] , Acc : 0.705607476635514\n",
      "Features : ['Al', 'Ca', 'Si'] , Acc : 0.6588785046728972\n",
      "Features : ['Al', 'Ca', 'K'] , Acc : 0.6728971962616822\n",
      "Features : ['Al', 'Ca', 'Ba'] , Acc : 0.6682242990654206\n",
      "Features : ['Al', 'Ca', 'Fe'] , Acc : 0.6588785046728972\n",
      "-------------------------------------------------------------------\n",
      "Pass3: best_feature_subset = ['Al', 'Ca', 'Mg'] , Accuracy = 0.705607476635514\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI'] , Acc : 0.7242990654205608\n",
      "Features : ['Al', 'Ca', 'Mg', 'Na'] , Acc : 0.7242990654205608\n",
      "Features : ['Al', 'Ca', 'Mg', 'Si'] , Acc : 0.705607476635514\n",
      "Features : ['Al', 'Ca', 'Mg', 'K'] , Acc : 0.7242990654205608\n",
      "Features : ['Al', 'Ca', 'Mg', 'Ba'] , Acc : 0.7102803738317757\n",
      "Features : ['Al', 'Ca', 'Mg', 'Fe'] , Acc : 0.705607476635514\n",
      "-------------------------------------------------------------------\n",
      "Pass4: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI'] , Accuracy = 0.7242990654205608\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Na'] , Acc : 0.7616822429906542\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Si'] , Acc : 0.7242990654205608\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'K'] , Acc : 0.7383177570093458\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Ba'] , Acc : 0.7757009345794392\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Fe'] , Acc : 0.7242990654205608\n",
      "-------------------------------------------------------------------\n",
      "Pass5: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Ba'] , Accuracy = 0.7757009345794392\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na'] , Acc : 0.7897196261682243\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Si'] , Acc : 0.7757009345794392\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'K'] , Acc : 0.7663551401869159\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Fe'] , Acc : 0.7757009345794392\n",
      "-------------------------------------------------------------------\n",
      "Pass6: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na'] , Accuracy = 0.7897196261682243\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na', 'Si'] , Acc : 0.7897196261682243\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na', 'K'] , Acc : 0.7616822429906542\n",
      "Features : ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na', 'Fe'] , Acc : 0.7897196261682243\n",
      "Acccuracy stop improving\n",
      "Final select features: ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na'], Accuracy = 0.7897196261682243\n"
     ]
    }
   ],
   "source": [
    "feature_selection_NB(entropy_base_df,X,'class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
