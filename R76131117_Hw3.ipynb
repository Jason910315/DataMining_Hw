{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from math import sqrt\n",
    "from math import log2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 txt 檔案\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = [row for row in reader]\n",
    "    return data\n",
    "# 讀取資料\n",
    "data = load_data('glass.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_column_dict(data, columns):\n",
    "    # 初始化一個空的 dictionary，key 是 attributes 名稱，value 是空 list：\n",
    "    df_dict = {col: [] for col in columns}\n",
    "    # 對每一列資料逐欄掃描，同時把欄位名稱 (col) 跟對應值 (val) 配對起來\n",
    "    for row in data:\n",
    "        for col, val in zip(columns, row):\n",
    "                val = float(val)\n",
    "                df_dict[col].append(val)\n",
    "    return df_dict\n",
    "# 定義欄位名稱\n",
    "columns = [\"Id\",\"RI\",\"Na\",\"Mg\",\"Al\",\"Si\",\"K\",\"Ca\",\"Ba\",\"Fe\",\"class\"]\n",
    "df = table_to_column_dict(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = columns.copy()\n",
    "X.remove(\"Id\")\n",
    "X.remove(\"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算 feature entropy\n",
    "def entropy(df,feature):  \n",
    "    att_value = df[feature]  # 取出 dict 的特定 attribute 的所有資料\n",
    "    value_count = Counter(att_value)          # 計算所有可能值的個數\n",
    "    total = len(att_value)\n",
    "    prob = [count / total for key,count in value_count.items()]  # 計算每個 attribute_value 的機率\n",
    "    return -sum(p * log2(p) for p in prob)\n",
    "\n",
    "# 計算特徵 X、Y 間的 Mutual Information\n",
    "def mutual_information(df,X, Y):\n",
    "    X_list = df[X]\n",
    "    Y_list = df[Y]\n",
    "    # 計算 X 和 Y 的熵\n",
    "    H_X = entropy(df ,X)\n",
    "    H_Y = entropy(df ,Y)\n",
    "    # 計算 X 和 Y 的聯合機率\n",
    "    joint_pairs = list(zip(X_list, Y_list))\n",
    "    joint_counts = Counter(joint_pairs)\n",
    "    total = len(X_list)\n",
    "    joint_prob = [count / total for key,count in joint_counts.items()]\n",
    "    H_X_Y = -sum(p * log2(p) for p in joint_prob)\n",
    "    return H_X + H_Y - H_X_Y\n",
    "    \n",
    "# 計算特徵 X、Y 的 symmetric uncertainty\n",
    "def cal_su(df,X,Y):\n",
    "    H_X = entropy(df,X)\n",
    "    H_Y = entropy(df,Y)\n",
    "    if H_X == 0 and H_Y == 0:\n",
    "        return 0\n",
    "    return 2 * (mutual_information(df,X,Y) / (H_X + H_Y))\n",
    "\n",
    "# 計算選取的特徵子集對於類別預測的 Goodness\n",
    "def Goodness(df,feature_subset,label):\n",
    "    su_X_C = 0\n",
    "    sum_su_X_Y = 0  \n",
    "    # 計算 feature_subset 內所有特徵對於類別值的 Symmetric uncertainty\n",
    "    su_X_C = sum(cal_su(df,X,label) for X in feature_subset)\n",
    "    \n",
    "    # 計算 feature_subset 內所有兩兩特徵間的 Symmetric uncertainty\n",
    "    for feature_i in feature_subset:\n",
    "        for feature_j in feature_subset:\n",
    "            sum_su_X_Y += cal_su(df,feature_i,feature_j)\n",
    "    if sum_su_X_Y == 0:\n",
    "        return 0\n",
    "    return su_X_C / sqrt(sum_su_X_Y)\n",
    "\n",
    "def forward_selection(df, X, y):\n",
    "    select_features = []    \n",
    "    best_score = 0.0        \n",
    "    remaining_features = X.copy()  \n",
    "    # 持續檢查直到沒有可以選擇的 feature\n",
    "    i = 1   \n",
    "    while(len(remaining_features) > 0):\n",
    "        scores = []  \n",
    "        for feature in remaining_features:\n",
    "            # temp_features 暫存此次循環的特徵組合 => 上回以選取好的最佳組合 select_features + 這回新選入的一個 feature\n",
    "            temp_features = select_features + [feature]\n",
    "            score = Goodness(df,temp_features,y)\n",
    "            # (目前的特徵組合, 新選進來的特徵, 此特徵組合的 Goodness)\n",
    "            scores.append((temp_features,feature,score))\n",
    "\n",
    "        # 依照 Goodness 排序\n",
    "        scores.sort(key=lambda x: x[2], reverse = True)  \n",
    "        best_new_score = float(scores[0][2])  \n",
    "        print(\"Forward Selection:\")\n",
    "        if(best_new_score > best_score):\n",
    "            best_score = best_new_score\n",
    "            select_features = scores[0][0]  # 更新成 Goodness 最優的 subset\n",
    "            if scores[0][1] in remaining_features:\n",
    "                remaining_features.remove(scores[0][1])  # 移除新選特徵\n",
    "            print(f\"Pass{i}: best_feature_subset = {select_features} , Goodness = {best_score}\")\n",
    "            i += 1\n",
    "        # 此輪中所有 feature_subset 的表現皆不如上一輪，Stop\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Final select features: {select_features}, Goodness = {best_score}\")\n",
    "\n",
    "def backward_selection(df, X, y):\n",
    "    select_features = X    \n",
    "    best_score = 0.0       \n",
    "    i = 1\n",
    "    # 持續檢查到選擇的 feature 只剩下一個\n",
    "    while(len(select_features) > 1):\n",
    "        scores = [] \n",
    "        for feature in select_features:\n",
    "            temp_features = select_features.copy()\n",
    "            # 每次移除一個 feature\n",
    "            temp_features.remove(feature)\n",
    "            score = Goodness(df,temp_features,y)\n",
    "            # (目前的特徵組合, 移除的特徵, 此特徵組合的 Goodness)\n",
    "            scores.append((temp_features,feature,score))\n",
    "\n",
    "        scores.sort(key = lambda x: x[2], reverse = True)  \n",
    "        best_new_score = float(scores[0][2]) \n",
    "        print(\"Backward Selection:\")\n",
    "        if(best_new_score >= best_score):\n",
    "            best_score = best_new_score\n",
    "            select_features = scores[0][0]  # 更新成 Goodness 最優的 subset\n",
    "            if scores[0][1] in select_features:\n",
    "                select_features.remove(scores[0][1])  # 移除特徵\n",
    "            print(f\"Pass{i}: best_feature_subset = {select_features} , Goodness = {best_score}\")\n",
    "            print(f\"remove feature: {scores[0][1]}\")\n",
    "            i += 1\n",
    "        # 此輪中所有 feature_subset 的表現皆不如上一輪，Stop\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Final select features: {select_features}, Goodness = {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with equal width discretization => width = 0.00228\n",
      "[1.513428, 1.515706, 1.517984, 1.520262, 1.52254, 1.524818, 1.527096, 1.529374, 1.531652]\n",
      "=========================================================\n",
      "Na with equal width discretization => width = 0.665\n",
      "[11.395, 12.06, 12.725, 13.39, 14.055, 14.719999999999999, 15.384999999999998, 16.049999999999997, 16.715]\n",
      "=========================================================\n",
      "Mg with equal width discretization => width = 0.449\n",
      "[0.449, 0.898, 1.347, 1.796, 2.245, 2.694, 3.1430000000000002, 3.592, 4.041]\n",
      "=========================================================\n",
      "Al with equal width discretization => width = 0.321\n",
      "[0.611, 0.9319999999999999, 1.2530000000000001, 1.574, 1.895, 2.216, 2.537, 2.858, 3.1790000000000003]\n",
      "=========================================================\n",
      "Si with equal width discretization => width = 0.56\n",
      "[70.37, 70.93, 71.49, 72.05, 72.61, 73.17, 73.73, 74.28999999999999, 74.85]\n",
      "=========================================================\n",
      "K with equal width discretization => width = 0.621\n",
      "[0.621, 1.242, 1.863, 2.484, 3.105, 3.726, 4.3469999999999995, 4.968, 5.589]\n",
      "=========================================================\n",
      "Ca with equal width discretization => width = 1.076\n",
      "[6.506, 7.582, 8.658, 9.734, 10.81, 11.886, 12.962, 14.038, 15.114]\n",
      "=========================================================\n",
      "Ba with equal width discretization => width = 0.315\n",
      "[0.315, 0.63, 0.9450000000000001, 1.26, 1.575, 1.8900000000000001, 2.205, 2.52, 2.835]\n",
      "=========================================================\n",
      "Fe with equal width discretization => width = 0.051\n",
      "[0.051000000000000004, 0.10200000000000001, 0.15300000000000002, 0.20400000000000001, 0.255, 0.30600000000000005, 0.35700000000000004, 0.40800000000000003, 0.459]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def equal_width(df,feature,bin_num):\n",
    "    # 計算每組 bin 區間\n",
    "    att_value = df[feature]\n",
    "    max_value = float(max(att_value))\n",
    "    min_value = float(min(att_value))\n",
    "    # 計算每組區間寬度\n",
    "    width = (max_value - min_value) / bin_num\n",
    "\n",
    "    bins = []  # 儲存每組區間範圍\n",
    "    # 計算每組區間數值範圍\n",
    "    for i in range(1, bin_num):\n",
    "        cut_point = min_value + i * width  # 取到小數點第5位\n",
    "        bins.append(cut_point)\n",
    "\n",
    "    print(f'{feature} with equal width discretization => width = {round(width, 5)}')\n",
    "    print(bins)\n",
    "    print(\"=========================================================\")\n",
    "    # 儲存 discretization 後的值\n",
    "    bin_result = []\n",
    "    for value in att_value:\n",
    "        value = float(value)\n",
    "        for i, cut_value in enumerate(bins):\n",
    "            if i == 0 and value <= cut_value:\n",
    "                bin_result.append(i + 1)\n",
    "                break\n",
    "            elif i == 0 and value > cut_value and value <= bins[i + 1]:\n",
    "                bin_result.append(i + 2)\n",
    "                break\n",
    "            elif i == bin_num - 2 and value > cut_value:\n",
    "                bin_result.append(i + 2)\n",
    "            elif i == bin_num - 2 and value <= cut_value and value > bin[i - 1]:\n",
    "                bin_result.append(i + 1)\n",
    "            elif value > cut_value and value <= bins[i + 1]:\n",
    "                bin_result.append(i + 2)\n",
    "                break\n",
    "    return bin_result\n",
    "\n",
    "def discretize_equal_width(df, features, bin_num):\n",
    "    new_df = []\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        bin_results[feature] = equal_width(df, feature, bin_num)\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "\n",
    "# 對原始資料所有連續型變數做離散化並存到新的 dict of list 裡\n",
    "equal_width_df = discretize_equal_width(df, X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection:\n",
      "Pass1: best_feature_subset = ['Ba'] , Goodness = 0.3042912813744425\n",
      "Forward Selection:\n",
      "Pass2: best_feature_subset = ['Ba', 'Mg'] , Goodness = 0.39168312637244257\n",
      "Forward Selection:\n",
      "Pass3: best_feature_subset = ['Ba', 'Mg', 'Ca'] , Goodness = 0.3973212810619197\n",
      "Forward Selection:\n",
      "Pass4: best_feature_subset = ['Ba', 'Mg', 'Ca', 'Na'] , Goodness = 0.405125062147965\n",
      "Forward Selection:\n",
      "Pass5: best_feature_subset = ['Ba', 'Mg', 'Ca', 'Na', 'Al'] , Goodness = 0.4111914788502334\n",
      "Forward Selection:\n",
      "Final select features: ['Ba', 'Mg', 'Ca', 'Na', 'Al'], Goodness = 0.4111914788502334\n",
      "=================================================================================================\n",
      "Backward Selection:\n",
      "Pass1: best_feature_subset = ['Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.3929712216460038\n",
      "remove feature: RI\n",
      "Backward Selection:\n",
      "Pass2: best_feature_subset = ['Na', 'Mg', 'Al', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.40295299406768176\n",
      "remove feature: Si\n",
      "Backward Selection:\n",
      "Pass3: best_feature_subset = ['Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'] , Goodness = 0.4094585798175886\n",
      "remove feature: Fe\n",
      "Backward Selection:\n",
      "Pass4: best_feature_subset = ['Na', 'Mg', 'Al', 'Ca', 'Ba'] , Goodness = 0.4111914788502334\n",
      "remove feature: K\n",
      "Backward Selection:\n",
      "Final select features: ['Na', 'Mg', 'Al', 'Ca', 'Ba'], Goodness = 0.4111914788502334\n"
     ]
    }
   ],
   "source": [
    "# 對所有特徵離散化後的資料集做 forward selection、backward selection\n",
    "forward_selection(equal_width_df, X, 'class')\n",
    "print(\"=================================================================================================\")\n",
    "backward_selection(equal_width_df, X, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with equal frequency discretization:\n",
      "[1.5159, 1.51629, 1.51667, 1.51732, 1.51766, 1.51808, 1.51852, 1.51977, 1.52177]\n",
      "=========================================================\n",
      "Na with equal frequency discretization:\n",
      "[12.67, 12.85, 13.0, 13.2, 13.33, 13.49, 13.73, 14.14, 14.56]\n",
      "=========================================================\n",
      "Mg with equal frequency discretization:\n",
      "[0.0, 2.72, 3.36, 3.48, 3.54, 3.59, 3.66, 3.82]\n",
      "=========================================================\n",
      "Al with equal frequency discretization:\n",
      "[0.83, 1.14, 1.23, 1.3, 1.38, 1.51, 1.58, 1.8, 2.12]\n",
      "=========================================================\n",
      "Si with equal frequency discretization:\n",
      "[71.77, 72.12, 72.38, 72.65, 72.78, 72.89, 73.01, 73.11, 73.28]\n",
      "=========================================================\n",
      "K with equal frequency discretization:\n",
      "[0.0, 0.11, 0.33, 0.52, 0.56, 0.58, 0.61, 0.66, 1.41]\n",
      "=========================================================\n",
      "Ca with equal frequency discretization:\n",
      "[7.96, 8.11, 8.32, 8.44, 8.6, 8.78, 9.02, 9.57, 10.88]\n",
      "=========================================================\n",
      "Ba with equal frequency discretization:\n",
      "[0.0, 0.76]\n",
      "=========================================================\n",
      "Fe with equal frequency discretization:\n",
      "[0.0, 0.11, 0.19, 0.32]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def equal_frequency(df,feature,bin_num):\n",
    "    # 先記錄每個 instance 的原始索引及 value，len(df[feature]) 為資料總筆數\n",
    "    att_value = [(i, float(df[feature][i])) for i in range(len(df[feature]))]\n",
    "    # 以 attribute value 的值排序\n",
    "    att_value.sort(key = lambda x : x[1])\n",
    "    # 計算每個 bin 應該包含的 instances 數量\n",
    "    frequency = len(att_value) // bin_num\n",
    "    bins = [] \n",
    "    start = att_value[0][1]\n",
    "    bin_index = 1    # 記錄目前 bin \n",
    "    cur_bin_cnt = 0  # 記錄目前 bin 所分配到的 value 個數\n",
    "    # att_value 已排序過，故會由小到大遍歷，org_index 是紀錄該筆 instance 在未排序前的位置\n",
    "    for cur,(org_index,value) in enumerate(att_value):\n",
    "        # 目前 bin 的 value 數量已滿足一個 bin 所應該分配到的 frequency\n",
    "        if cur_bin_cnt >= frequency and bin_index <= bin_num:\n",
    "            # 且當前 value 不等於前一個 value 值\n",
    "            if cur < len(att_value) and value != att_value[cur - 1][1]:\n",
    "                # 若已計算到最後一個 bin\n",
    "                if bin_index == bin_num:\n",
    "                    bins.append((start,att_value[-1][1]))  # end 即為最後一筆 instance(最大值)\n",
    "                    break\n",
    "                end = att_value[cur - 1][1]  # 該 bin 的區間最大值(包含)\n",
    "                bins.append((start,end))\n",
    "\n",
    "                # 切換到下一個 bin\n",
    "                bin_index += 1\n",
    "                cur_bin_cnt = 0\n",
    "                start = value  # att_value[i][1] 為下個 bin 的起點\n",
    "        # 該 bin 裡的 instances 個數加一\n",
    "        cur_bin_cnt += 1\n",
    "\n",
    "    # 上述設定在切換下個 bin 時才將 (start,end) 進 bins\n",
    "    # 有可能迴圈結束，最後一個 bin 的值個數不足一個 frequency，不會切換 bin，因此需要額外判斷防止最後一個 bin 消失\n",
    "    if len(bins) < bin_num:\n",
    "        end = att_value[-1][1]   # end 為最後一個元素(最大值)\n",
    "        bins.append((start, end))\n",
    "    print(f'{feature} with equal frequency discretization:')\n",
    "    print_bins = []\n",
    "    for i, (start, end) in enumerate(bins):\n",
    "        if i == len(bins) - 1:\n",
    "            break\n",
    "        print_bins.append(end)\n",
    "    print(print_bins)\n",
    "    print(\"=========================================================\")\n",
    "    # 對 df 做離散化並將新值存到一個 dict of list\n",
    "    org_value = df[feature]\n",
    "    bin_result = []\n",
    "    for value in org_value:\n",
    "        value = float(value)\n",
    "        for i, (start, end) in enumerate(bins):\n",
    "            if i == 0 and value >= start and value <= end:\n",
    "                bin_result.append(i + 1)\n",
    "                break\n",
    "            elif value >= start and value <= end:\n",
    "                bin_result.append(i + 1)\n",
    "                break\n",
    "    return bin_result\n",
    "\n",
    "\n",
    "def discretize_equal_frequency(df, features, bin_num):\n",
    "    new_df = []\n",
    "    # 對每個 feature 做離散化，回傳 bin 結果(dict of list）\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        bin_results[feature] = equal_frequency(df, feature, bin_num)\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "\n",
    "equal_frequency_df = discretize_equal_frequency(df,X,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection:\n",
      "Pass1: best_feature_subset = ['Mg'] , Goodness = 0.24851240850960837\n",
      "Forward Selection:\n",
      "Pass2: best_feature_subset = ['Mg', 'Ba'] , Goodness = 0.3223383424643906\n",
      "Forward Selection:\n",
      "Pass3: best_feature_subset = ['Mg', 'Ba', 'Al'] , Goodness = 0.3611469799776129\n",
      "Forward Selection:\n",
      "Pass4: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI'] , Goodness = 0.3833761038884968\n",
      "Forward Selection:\n",
      "Pass5: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI', 'K'] , Goodness = 0.3960303164266635\n",
      "Forward Selection:\n",
      "Pass6: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI', 'K', 'Ca'] , Goodness = 0.39697499942233605\n",
      "Forward Selection:\n",
      "Pass7: best_feature_subset = ['Mg', 'Ba', 'Al', 'RI', 'K', 'Ca', 'Na'] , Goodness = 0.3973256378646354\n",
      "Forward Selection:\n",
      "Final select features: ['Mg', 'Ba', 'Al', 'RI', 'K', 'Ca', 'Na'], Goodness = 0.3973256378646354\n",
      "=================================================================================================\n",
      "Backward Selection:\n",
      "Pass1: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.39160854990249794\n",
      "remove feature: Si\n",
      "Backward Selection:\n",
      "Pass2: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'] , Goodness = 0.3973256378646354\n",
      "remove feature: Fe\n",
      "Backward Selection:\n",
      "Final select features: ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'], Goodness = 0.3973256378646354\n"
     ]
    }
   ],
   "source": [
    "# 對所有特徵離散化後的資料集做 forward selection、backward selection\n",
    "forward_selection(equal_frequency_df, X, 'class')\n",
    "print(\"=================================================================================================\")\n",
    "backward_selection(equal_frequency_df, X, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算區間內類別值的 entropy\n",
    "def Ent(df,class_label):\n",
    "    class_value = df[class_label]\n",
    "    cnt = Counter(class_value)\n",
    "    prob = [count / len(class_value) for i,count in cnt.items()]\n",
    "    Ent = -sum(p * log2(p) for p in prob) \n",
    "    return Ent\n",
    "\n",
    "# cut_index 為資料被分割成兩個 subset 時，「右側區間的起始 index」，此函式計算切割後的資訊增益\n",
    "def info_gain(df, cutpoint, feature, class_label):\n",
    "    total = len(df[class_label])\n",
    "\n",
    "    # 依據 feature 值是否 <= midpoint 來分割資料\n",
    "    left = {class_label: [df[class_label][i] for i in range(len(df[feature])) if float(df[feature][i]) <= cutpoint]}\n",
    "    right = {class_label: [df[class_label][i] for i in range(len(df[feature])) if float(df[feature][i]) > cutpoint]}\n",
    "\n",
    "    # 若任一邊為空，代表不是有效切點，資訊增益為 0\n",
    "    if not left[class_label] or not right[class_label]:\n",
    "        return 0\n",
    "\n",
    "    Ent_cut = (len(left[class_label]) / total) * Ent(left, class_label) + (len(right[class_label]) / total) * Ent(right, class_label)\n",
    "\n",
    "    return Ent(df, class_label) - Ent_cut\n",
    "\n",
    "# 找 feature 的最佳切點\n",
    "def find_cut_point(df, feature, class_label):\n",
    "    best_info_gain = -1\n",
    "    best_cut_value = None\n",
    "\n",
    "    # 整理為 (value, label) 配對並排序\n",
    "    value_label = sorted([(float(df[feature][i]), df[class_label][i], i) for i in range(len(df[feature]))], key=lambda x: (x[0], x[2]))\n",
    "    for i in range(1, len(value_label)):\n",
    "        if value_label[i][1] != value_label[i - 1][1]:  # 類別不同才考慮切點\n",
    "            midpoint = (value_label[i][0] + value_label[i - 1][0]) / 2\n",
    "            cur_info_gain = info_gain(df, midpoint, feature, class_label)\n",
    "            if cur_info_gain > best_info_gain:\n",
    "                \n",
    "                best_info_gain = cur_info_gain\n",
    "                best_cut_value = midpoint\n",
    "\n",
    "    return best_info_gain, best_cut_value\n",
    "\n",
    "# 對整個 df 的 feature 欄位做 entropy_base 切割，找所有切點，返回切割點 list\n",
    "def split(df,feature,class_label,cut_points):\n",
    "    # 若傳進來的 cut_points 為空，代表區間無可用的切割點\n",
    "    if cut_points is None:\n",
    "        cut_points = []\n",
    "    best_info_gain,best_cut_value = find_cut_point(df,feature,class_label)\n",
    "\n",
    "    # 若區間的 class 值或 feature 值都是一樣的，或只剩一個 instance，代表切割無意義\n",
    "    if (len(df[class_label]) <= 1 or best_cut_value is None or\n",
    "        len(set(df[class_label])) == 1 or len(set(df[feature])) == 1):\n",
    "        return []\n",
    "    \n",
    "    # 創造兩個 dict of list 儲存切割後的 feature 與 class 欄位\n",
    "    left_set = {feature : [],class_label : []}\n",
    "    right_set = {feature : [],class_label : []}\n",
    "    for i in range(len(df[feature])):\n",
    "        # 分配 instances 至對應的區間\n",
    "        value = float(df[feature][i])\n",
    "        if value <= best_cut_value:\n",
    "            left_set[feature].append(value)\n",
    "            left_set[class_label].append(df[class_label][i])\n",
    "        else:\n",
    "            right_set[feature].append(value)\n",
    "            right_set[class_label].append(df[class_label][i])\n",
    "    # 切割完，若任一區間沒有資料，或分割後區間內容與分割前一樣，代表分割沒有幫助 \n",
    "    if (len(left_set[feature]) == 0 or len(right_set[feature]) == 0 or\n",
    "        len(left_set[feature]) == len(df[feature]) or len(right_set[feature]) == len(df[feature])):\n",
    "        return []\n",
    "    # 計算 MDLPC criterion 的 threshold\n",
    "\n",
    "    # 計算初始區間、切割後的左右區間個包含的 class 種類數量\n",
    "    k = len(set(df[class_label]))\n",
    "    k1 = len(set(left_set[class_label]))\n",
    "    k2 = len(set(right_set[class_label]))\n",
    "    N = len(df[feature])\n",
    "    H_S = Ent(df, class_label)\n",
    "    H_l = Ent(left_set, class_label)\n",
    "    H_r = Ent(right_set, class_label)\n",
    "    delta = log2(3 ** k - 2) - (k * H_S - k1 * H_l - k2 * H_r)\n",
    "    threshold = log2(N - 1) / N + delta / N\n",
    "\n",
    "    # 最好的切割點的 gain 未超過 threshold，停止\n",
    "    if best_info_gain <= threshold :\n",
    "        return []\n",
    "    \n",
    "    cut_points.append(best_cut_value)\n",
    "    # 對左右區間遞迴做 entropy_base 切割\n",
    "    split(left_set,feature,class_label,cut_points)\n",
    "    split(right_set,feature,class_label,cut_points)\n",
    "    return cut_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI with entropy base discretization:\n",
      "[1.517335, 1.517985]\n",
      "=========================================================\n",
      "Na with entropy base discretization:\n",
      "[14.065]\n",
      "=========================================================\n",
      "Mg with entropy base discretization:\n",
      "[2.6950000000000003]\n",
      "=========================================================\n",
      "Al with entropy base discretization:\n",
      "[1.38, 1.76]\n",
      "=========================================================\n",
      "Si with entropy base discretization:\n",
      "[]\n",
      "=========================================================\n",
      "K with entropy base discretization:\n",
      "[0.055, 0.61, 0.745]\n",
      "=========================================================\n",
      "Ca with entropy base discretization:\n",
      "[7.02, 8.28, 10.075]\n",
      "=========================================================\n",
      "Ba with entropy base discretization:\n",
      "[0.335]\n",
      "=========================================================\n",
      "Fe with entropy base discretization:\n",
      "[]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# 照上述的切點做 discretization\n",
    "def discretize_entropy_base(df,features,class_label):\n",
    "    bin_results = {}\n",
    "    for feature in features:\n",
    "        print(f'{feature} with entropy base discretization:')    \n",
    "        cut_points = []\n",
    "        cut_points = split(df,feature,\"class\",cut_points)\n",
    "        # 若沒有切點，代表整個特徵內容會被離散為同個類別\n",
    "        if cut_points == []:\n",
    "            ent_base_res = [1] * len(df[feature])\n",
    "            bin_results[feature] = ent_base_res\n",
    "        else:\n",
    "            cut_points = sorted(cut_points)\n",
    "            org_value = df[feature]     # 紀錄特徵值的原始值\n",
    "            ent_base_res = []           # 紀錄離散化後的特徵值\n",
    "            # 對每個 feature 值做離散化分配\n",
    "            for value in org_value:\n",
    "                value = float(value)\n",
    "                for i,cut_point in enumerate(cut_points):\n",
    "                    # 只有一個切割點，只會被切成兩個區間\n",
    "                    if len(cut_points) == 1:\n",
    "                        if value <= cut_point:\n",
    "                            ent_base_res.append(i + 1)\n",
    "                        else:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                    # 有兩個以上的切點\n",
    "                    else:\n",
    "                        if i == 0 and value <= cut_point:\n",
    "                            ent_base_res.append(i + 1)\n",
    "                        elif i == 0 and value > cut_point and value <= cut_points[i + 1]:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                        elif i == len(cut_points) - 1 and value > cut_point:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                        elif value > cut_point and value <= cut_points[i + 1]:\n",
    "                            ent_base_res.append(i + 2)\n",
    "                       \n",
    "            bin_results[feature] = ent_base_res\n",
    "        print(f\"{cut_points}\")\n",
    "        print(\"=========================================================\")\n",
    "    bin_results['class'] = df['class']\n",
    "    return bin_results\n",
    "entropy_base_df = discretize_entropy_base(df,X,'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Selection:\n",
      "Pass1: best_feature_subset = ['Mg'] , Goodness = 0.37040112253047947\n",
      "Forward Selection:\n",
      "Pass2: best_feature_subset = ['Mg', 'Al'] , Goodness = 0.44111979963245035\n",
      "Forward Selection:\n",
      "Pass3: best_feature_subset = ['Mg', 'Al', 'Ca'] , Goodness = 0.4684573941293161\n",
      "Forward Selection:\n",
      "Pass4: best_feature_subset = ['Mg', 'Al', 'Ca', 'Ba'] , Goodness = 0.4923013144646698\n",
      "Forward Selection:\n",
      "Pass5: best_feature_subset = ['Mg', 'Al', 'Ca', 'Ba', 'K'] , Goodness = 0.5084334732749055\n",
      "Forward Selection:\n",
      "Final select features: ['Mg', 'Al', 'Ca', 'Ba', 'K'], Goodness = 0.5084334732749055\n",
      "=================================================================================================\n",
      "Backward Selection:\n",
      "Pass1: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba', 'Fe'] , Goodness = 0.5106082375130458\n",
      "remove feature: Si\n",
      "Backward Selection:\n",
      "Pass2: best_feature_subset = ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'] , Goodness = 0.5106082375130458\n",
      "remove feature: Fe\n",
      "Backward Selection:\n",
      "Final select features: ['RI', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Ba'], Goodness = 0.5106082375130458\n"
     ]
    }
   ],
   "source": [
    "# 對所有特徵離散化後的資料集做 forward selection、backward selection\n",
    "forward_selection(entropy_base_df, X, 'class')\n",
    "print(\"=================================================================================================\")\n",
    "backward_selection(entropy_base_df, X, 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### equal_width_df、equal_frequency_df、entropy_base_df 為所有特徵皆以離散化後的 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算 P(feature_name = X_i | class_name = C_j) 的 Laplace Estimation 值\n",
    "def laplace_est(df, feature_name, class_name, X_i, C_j):\n",
    "    # 取出特定特徵與類別的所有值\n",
    "    feature_values = df[feature_name]\n",
    "    class_values = df[class_name]\n",
    "    # 取出當 class = C_j 類別時的 feature_name 可能值 f\n",
    "    filter_feature_Cj = [f for f, C in zip(feature_values, class_values) if C == C_j]\n",
    "    # N_ij 為 feature_name 值 = X_i | class_name = C_j 的樣本個數\n",
    "    N_ij = filter_feature_Cj.count(X_i)\n",
    "    # N_j 是類別為 Cj 的樣本總數\n",
    "    N_j = len(filter_feature_Cj)\n",
    "    # k 為特徵 feature_name 的可能值個數\n",
    "    k = len(set(feature_values))\n",
    "    return (N_ij + 1) / (N_j + k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以 naive bayes 預測資料\n",
    "def naive_bayes(df,feature_set,class_name):\n",
    "    prediction = [] # 儲存資料集預測結果\n",
    "    # 計算每個類別的先驗機率 P(class_name = C_j)\n",
    "    N = len(df[class_name])\n",
    "    # 取出類別的可能值\n",
    "    class_values = list(set(df[class_name]))\n",
    "    # 計算每個唯一類別值的先驗機率\n",
    "    prior_C = {c : df[class_name].count(c) / N for c in class_values}\n",
    "    # 透過 feature_set 的值預測每筆樣本\n",
    "    # 每筆資料都存在一組 X (每個特徵的值)，針對每個資料的所有特徵值計算 P( Xi | Cj)\n",
    "    for i in range(N):\n",
    "        instance = {f : df[f][i] for f in feature_set} \n",
    "        max_prob = -1  # 儲存類別的預測可能機率\n",
    "        predict_c = None\n",
    "        # 針對每個類別 Cj 計算 SumProduct(P(Xi | Cj)*P(Cj))\n",
    "        for Cj in class_values:\n",
    "            p_Xi_Cj = 1\n",
    "            # 計算每個 feature 對應的 P(Xi | Cj)\n",
    "            for feature in feature_set:\n",
    "                p_Xi_Cj *= laplace_est(df,feature,class_name,instance[feature],Cj)\n",
    "            # 最後計算後驗機率\n",
    "            posterior = prior_C[Cj] * p_Xi_Cj\n",
    "            # 若此類別的後驗機率 > 先前的最大機率\n",
    "            if posterior > max_prob:\n",
    "                max_prob = posterior\n",
    "                predict_c = Cj  # 改為預測 Cj 類別\n",
    "        prediction.append(predict_c)   \n",
    "    return prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = naive_bayes(df,X,'class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算預測準確率\n",
    "def cal_accuracy(df,class_name,prediction):\n",
    "    correct = 0\n",
    "    for i in range(len(df[class_name])):\n",
    "        if df[class_name][i] == prediction[i]:\n",
    "            correct += 1\n",
    "    return correct / len(df[class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9906542056074766"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_accuracy(df,'class',pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"RI\",\"Na\",\"Mg\",\"Al\",\"Si\",\"K\",\"Ca\",\"Ba\",\"Fe\",\"class\"]\n",
    "X = columns.copy()\n",
    "X.remove(\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 Naive Bayse 方法執行 Feature Selection\n",
    "def feature_selection_NB(df,features,class_name):\n",
    "    select_features = []\n",
    "    remaining_features = features.copy()\n",
    "    best_accuracy = 0.0\n",
    "    class_values = df[class_name]\n",
    "    N = len(class_values)\n",
    "    i = 1\n",
    "    # 當還有特徵未被選取\n",
    "    while len(remaining_features) > 0:\n",
    "        Accuracy = [] # 儲存該輪每個特徵加進來後的個別準確率\n",
    "        # 測試每個還未被選取的特徵\n",
    "        \n",
    "        for feature in remaining_features:\n",
    "            temp_features = select_features + [feature]\n",
    "            # 使用當前特徵 temp_features 去做 naive bayse 預測\n",
    "            prediction = naive_bayes(df,temp_features,class_name)\n",
    "            acc = cal_accuracy(df,class_name,prediction)\n",
    "            Accuracy.append((feature,acc))\n",
    "        print(Accuracy)\n",
    "\n",
    "        # 依照準確率由大到小排序\n",
    "        Accuracy.sort(key = lambda x : x[1],reverse = True)\n",
    "        best_feature, best_new_acc = Accuracy[0][0],Accuracy[0][1]  # 得出該輪準確率最好的特徵與 Accuracy\n",
    "        print(\"Forward Selelcton:\")\n",
    "        # 若準確率大於上輪最優準確率\n",
    "        if best_new_acc > best_accuracy:\n",
    "            best_accuracy = best_new_acc\n",
    "            select_features = select_features + [best_feature]      # 選該特徵進來\n",
    "            remaining_features.remove(best_feature)\n",
    "            print(f\"Pass{i}: best_feature_subset = {select_features} , Accuracy = {best_accuracy}\")\n",
    "            i += 1\n",
    "        # 當準確率無法再提升即停止\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Final select features: {select_features}, Accuracy = {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RI', 0.411214953271028), ('Na', 0.46261682242990654), ('Mg', 0.4766355140186916), ('Al', 0.5186915887850467), ('Si', 0.42990654205607476), ('K', 0.4485981308411215), ('Ca', 0.48598130841121495), ('Ba', 0.4766355140186916), ('Fe', 0.37850467289719625)]\n",
      "Forward Selelcton:\n",
      "Pass1: best_feature_subset = ['Al'] , Accuracy = 0.5186915887850467\n",
      "[('RI', 0.5327102803738317), ('Na', 0.5467289719626168), ('Mg', 0.5420560747663551), ('Si', 0.5420560747663551), ('K', 0.5560747663551402), ('Ca', 0.602803738317757), ('Ba', 0.5560747663551402), ('Fe', 0.5420560747663551)]\n",
      "Forward Selelcton:\n",
      "Pass2: best_feature_subset = ['Al', 'Ca'] , Accuracy = 0.602803738317757\n",
      "[('RI', 0.6074766355140186), ('Na', 0.6261682242990654), ('Mg', 0.6635514018691588), ('Si', 0.602803738317757), ('K', 0.5887850467289719), ('Ba', 0.6355140186915887), ('Fe', 0.6308411214953271)]\n",
      "Forward Selelcton:\n",
      "Pass3: best_feature_subset = ['Al', 'Ca', 'Mg'] , Accuracy = 0.6635514018691588\n",
      "[('RI', 0.6728971962616822), ('Na', 0.6588785046728972), ('Si', 0.6495327102803738), ('K', 0.6448598130841121), ('Ba', 0.6728971962616822), ('Fe', 0.6588785046728972)]\n",
      "Forward Selelcton:\n",
      "Pass4: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI'] , Accuracy = 0.6728971962616822\n",
      "[('Na', 0.6962616822429907), ('Si', 0.6588785046728972), ('K', 0.6822429906542056), ('Ba', 0.6869158878504673), ('Fe', 0.6682242990654206)]\n",
      "Forward Selelcton:\n",
      "Pass5: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Na'] , Accuracy = 0.6962616822429907\n",
      "[('Si', 0.6869158878504673), ('K', 0.7009345794392523), ('Ba', 0.6915887850467289), ('Fe', 0.6915887850467289)]\n",
      "Forward Selelcton:\n",
      "Pass6: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K'] , Accuracy = 0.7009345794392523\n",
      "[('Si', 0.6915887850467289), ('Ba', 0.7102803738317757), ('Fe', 0.719626168224299)]\n",
      "Forward Selelcton:\n",
      "Pass7: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K', 'Fe'] , Accuracy = 0.719626168224299\n",
      "[('Si', 0.7102803738317757), ('Ba', 0.719626168224299)]\n",
      "Forward Selelcton:\n",
      "Final select features: ['Al', 'Ca', 'Mg', 'RI', 'Na', 'K', 'Fe'], Accuracy = 0.719626168224299\n"
     ]
    }
   ],
   "source": [
    "feature_selection_NB(equal_width_df,X,'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RI': [9,\n",
       "  5,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  10,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  1,\n",
       "  8,\n",
       "  6,\n",
       "  10,\n",
       "  10,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  10,\n",
       "  6,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  8,\n",
       "  6,\n",
       "  7,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  8,\n",
       "  7,\n",
       "  5,\n",
       "  4,\n",
       "  7,\n",
       "  3,\n",
       "  4,\n",
       "  7,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  7,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  7,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  9,\n",
       "  8,\n",
       "  3,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  6,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  7,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  9,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  10,\n",
       "  1,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  3,\n",
       "  9,\n",
       "  10,\n",
       "  1,\n",
       "  1,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  7,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  9,\n",
       "  3,\n",
       "  4],\n",
       " 'Na': [7,\n",
       "  8,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  5,\n",
       "  4,\n",
       "  8,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  9,\n",
       "  8,\n",
       "  4,\n",
       "  2,\n",
       "  10,\n",
       "  2,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  9,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  7,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  8,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  4,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  7,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  10,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  9,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  9,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  4,\n",
       "  6,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  9,\n",
       "  7,\n",
       "  6,\n",
       "  3,\n",
       "  4,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  9,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  7,\n",
       "  9,\n",
       "  6,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  1,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  10,\n",
       "  9,\n",
       "  9,\n",
       "  9],\n",
       " 'Mg': [9,\n",
       "  7,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  5,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  8,\n",
       "  8,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  9,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  8,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  6,\n",
       "  5,\n",
       "  3,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  7,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  4,\n",
       "  3,\n",
       "  8,\n",
       "  7,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  9,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  8,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  8,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'Al': [2,\n",
       "  5,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  8,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  8,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  8,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  9,\n",
       "  9,\n",
       "  6,\n",
       "  4,\n",
       "  7,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  2,\n",
       "  6,\n",
       "  7,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  8,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  9,\n",
       "  9,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  5,\n",
       "  6,\n",
       "  9,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  8,\n",
       "  6,\n",
       "  8,\n",
       "  3,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  8,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  2,\n",
       "  6,\n",
       "  8,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  7,\n",
       "  8,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  10,\n",
       "  9,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  10,\n",
       "  10,\n",
       "  6,\n",
       "  10,\n",
       "  6,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  1,\n",
       "  1,\n",
       "  9,\n",
       "  10,\n",
       "  3,\n",
       "  9,\n",
       "  5,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  3,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9],\n",
       " 'Si': [2,\n",
       "  5,\n",
       "  7,\n",
       "  4,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  2,\n",
       "  7,\n",
       "  9,\n",
       "  7,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  9,\n",
       "  8,\n",
       "  1,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  6,\n",
       "  8,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  8,\n",
       "  9,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  7,\n",
       "  10,\n",
       "  7,\n",
       "  5,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  9,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  10,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  8,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  3,\n",
       "  5,\n",
       "  9,\n",
       "  4,\n",
       "  6,\n",
       "  9,\n",
       "  3,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  9,\n",
       "  3,\n",
       "  3,\n",
       "  10,\n",
       "  9,\n",
       "  7,\n",
       "  9,\n",
       "  6,\n",
       "  10,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  10,\n",
       "  9,\n",
       "  8,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  6,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  8,\n",
       "  7,\n",
       "  10,\n",
       "  9,\n",
       "  4,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  1,\n",
       "  7,\n",
       "  5,\n",
       "  4,\n",
       "  7,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  6,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  10,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  10,\n",
       "  4,\n",
       "  5,\n",
       "  10,\n",
       "  10,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  8,\n",
       "  10,\n",
       "  7,\n",
       "  8,\n",
       "  6,\n",
       "  6,\n",
       "  4,\n",
       "  8,\n",
       "  10,\n",
       "  10,\n",
       "  10],\n",
       " 'K': [2,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  5,\n",
       "  8,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  9,\n",
       "  7,\n",
       "  9,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  7,\n",
       "  8,\n",
       "  4,\n",
       "  8,\n",
       "  7,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  8,\n",
       "  2,\n",
       "  2,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  5,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  9,\n",
       "  7,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  4,\n",
       "  9,\n",
       "  9,\n",
       "  7,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  4,\n",
       "  4,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  9,\n",
       "  4,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  9,\n",
       "  6,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  8,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  6,\n",
       "  9,\n",
       "  8,\n",
       "  6,\n",
       "  8,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  3,\n",
       "  3,\n",
       "  10,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  4,\n",
       "  9,\n",
       "  4,\n",
       "  3,\n",
       "  10,\n",
       "  10,\n",
       "  3,\n",
       "  9,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  10,\n",
       "  10,\n",
       "  7,\n",
       "  9,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  10,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  9,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'Ca': [6,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  8,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  9,\n",
       "  9,\n",
       "  7,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  7,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  8,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  9,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  4,\n",
       "  8,\n",
       "  7,\n",
       "  3,\n",
       "  7,\n",
       "  5,\n",
       "  6,\n",
       "  9,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  1,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  1,\n",
       "  1,\n",
       "  10,\n",
       "  9,\n",
       "  10,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  7,\n",
       "  4,\n",
       "  7,\n",
       "  4,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  6],\n",
       " 'Ba': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3],\n",
       " 'Fe': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'class': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  3.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  6.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0,\n",
       "  7.0]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equal_frequency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RI', 0.5373831775700935), ('Na', 0.49065420560747663), ('Mg', 0.514018691588785), ('Al', 0.5841121495327103), ('Si', 0.46261682242990654), ('K', 0.5327102803738317), ('Ca', 0.514018691588785), ('Ba', 0.4485981308411215), ('Fe', 0.37850467289719625)]\n",
      "Forward Selelcton:\n",
      "Pass1: best_feature_subset = ['Al'] , Accuracy = 0.5841121495327103\n",
      "[('RI', 0.6588785046728972), ('Na', 0.6308411214953271), ('Mg', 0.6495327102803738), ('Si', 0.6121495327102804), ('K', 0.6588785046728972), ('Ca', 0.6542056074766355), ('Ba', 0.6214953271028038), ('Fe', 0.6074766355140186)]\n",
      "Forward Selelcton:\n",
      "Pass2: best_feature_subset = ['Al', 'RI'] , Accuracy = 0.6588785046728972\n",
      "[('Na', 0.7383177570093458), ('Mg', 0.7336448598130841), ('Si', 0.6962616822429907), ('K', 0.7336448598130841), ('Ca', 0.7710280373831776), ('Ba', 0.705607476635514), ('Fe', 0.677570093457944)]\n",
      "Forward Selelcton:\n",
      "Pass3: best_feature_subset = ['Al', 'RI', 'Ca'] , Accuracy = 0.7710280373831776\n",
      "[('Na', 0.8130841121495327), ('Mg', 0.780373831775701), ('Si', 0.8177570093457944), ('K', 0.780373831775701), ('Ba', 0.7850467289719626), ('Fe', 0.7663551401869159)]\n",
      "Forward Selelcton:\n",
      "Pass4: best_feature_subset = ['Al', 'RI', 'Ca', 'Si'] , Accuracy = 0.8177570093457944\n",
      "[('Na', 0.822429906542056), ('Mg', 0.7990654205607477), ('K', 0.8271028037383178), ('Ba', 0.822429906542056), ('Fe', 0.8177570093457944)]\n",
      "Forward Selelcton:\n",
      "Pass5: best_feature_subset = ['Al', 'RI', 'Ca', 'Si', 'K'] , Accuracy = 0.8271028037383178\n",
      "[('Na', 0.8084112149532711), ('Mg', 0.8037383177570093), ('Ba', 0.8411214953271028), ('Fe', 0.8364485981308412)]\n",
      "Forward Selelcton:\n",
      "Pass6: best_feature_subset = ['Al', 'RI', 'Ca', 'Si', 'K', 'Ba'] , Accuracy = 0.8411214953271028\n",
      "[('Na', 0.8177570093457944), ('Mg', 0.8411214953271028), ('Fe', 0.8411214953271028)]\n",
      "Forward Selelcton:\n",
      "Final select features: ['Al', 'RI', 'Ca', 'Si', 'K', 'Ba'], Accuracy = 0.8411214953271028\n"
     ]
    }
   ],
   "source": [
    "feature_selection_NB(equal_frequency_df,X,'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RI', 0.5), ('Na', 0.4532710280373832), ('Mg', 0.4485981308411215), ('Al', 0.5887850467289719), ('Si', 0.35514018691588783), ('K', 0.5093457943925234), ('Ca', 0.5280373831775701), ('Ba', 0.4719626168224299), ('Fe', 0.35514018691588783)]\n",
      "Forward Selelcton:\n",
      "Pass1: best_feature_subset = ['Al'] , Accuracy = 0.5887850467289719\n",
      "[('RI', 0.6074766355140186), ('Na', 0.6214953271028038), ('Mg', 0.6448598130841121), ('Si', 0.5887850467289719), ('K', 0.6214953271028038), ('Ca', 0.6588785046728972), ('Ba', 0.616822429906542), ('Fe', 0.5887850467289719)]\n",
      "Forward Selelcton:\n",
      "Pass2: best_feature_subset = ['Al', 'Ca'] , Accuracy = 0.6588785046728972\n",
      "[('RI', 0.7009345794392523), ('Na', 0.6822429906542056), ('Mg', 0.705607476635514), ('Si', 0.6588785046728972), ('K', 0.6728971962616822), ('Ba', 0.6682242990654206), ('Fe', 0.6588785046728972)]\n",
      "Forward Selelcton:\n",
      "Pass3: best_feature_subset = ['Al', 'Ca', 'Mg'] , Accuracy = 0.705607476635514\n",
      "[('RI', 0.7242990654205608), ('Na', 0.7242990654205608), ('Si', 0.705607476635514), ('K', 0.7242990654205608), ('Ba', 0.7102803738317757), ('Fe', 0.705607476635514)]\n",
      "Forward Selelcton:\n",
      "Pass4: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI'] , Accuracy = 0.7242990654205608\n",
      "[('Na', 0.7616822429906542), ('Si', 0.7242990654205608), ('K', 0.7383177570093458), ('Ba', 0.7757009345794392), ('Fe', 0.7242990654205608)]\n",
      "Forward Selelcton:\n",
      "Pass5: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Ba'] , Accuracy = 0.7757009345794392\n",
      "[('Na', 0.7897196261682243), ('Si', 0.7757009345794392), ('K', 0.7663551401869159), ('Fe', 0.7757009345794392)]\n",
      "Forward Selelcton:\n",
      "Pass6: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na'] , Accuracy = 0.7897196261682243\n",
      "[('Si', 0.7897196261682243), ('K', 0.7616822429906542), ('Fe', 0.7897196261682243)]\n",
      "Forward Selelcton:\n",
      "Pass7: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na', 'Si'] , Accuracy = 0.7897196261682243\n",
      "[('K', 0.7616822429906542), ('Fe', 0.7897196261682243)]\n",
      "Forward Selelcton:\n",
      "Pass8: best_feature_subset = ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na', 'Si', 'Fe'] , Accuracy = 0.7897196261682243\n",
      "[('K', 0.7616822429906542)]\n",
      "Forward Selelcton:\n",
      "Final select features: ['Al', 'Ca', 'Mg', 'RI', 'Ba', 'Na', 'Si', 'Fe'], Accuracy = 0.7897196261682243\n"
     ]
    }
   ],
   "source": [
    "feature_selection_NB(entropy_base_df,X,'class')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
